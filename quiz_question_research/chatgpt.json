{
"exam": "AWS Certified Developer â€“ Associate (DVA-C02)",
"question_bank_version": "v1.0",
"generated_at": "2026-01-11T00:00:00Z",
"domains": [
{
"domain_id": "domain-1-development",
"name": "Development with AWS Services",
"topics": [
{
"topic_id": "lambda",
"name": "AWS Lambda",
"subtopics": [
{
"subtopic_id": "lambda-concurrency",
"name": "Lambda concurrency and scaling",
"num_questions_generated": 10,
"questions": [
{
"id": "q-d1-lc-001",
"concept_id": "c-lc-sqs-scaling",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-1-development",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A developer is building a data processing application where messages are published to an Amazon SQS standard queue and processed by an AWS Lambda function. The downstream database can handle only a limited number of concurrent writes. Which Lambda configuration will help the developer control the number of concurrent Lambda executions that process messages from the queue?",
"options": [
{ "label": "A", "text": "Set a reserved concurrency limit on the Lambda function." },
{ "label": "B", "text": "Increase the function memory size to reduce concurrency." },
{ "label": "C", "text": "Enable Lambda Destinations for asynchronous invocations." },
{ "label": "D", "text": "Configure a dead-letter queue on the Lambda function." }
],
"correct_options": ["A"],
"answer_explanation": "Reserved concurrency sets a maximum number of concurrent executions for a specific Lambda function. When used with an SQS event source, this ensures that Lambda will not process more messages in parallel than the reserved limit, protecting the downstream database from overload. Increasing memory size affects CPU allocation and performance but does not directly cap concurrency. Lambda Destinations handle the result of asynchronous invocations, not concurrency. Dead-letter queues handle failed invocations, not the number of concurrent executions.",
"why_this_matters": "Limiting Lambda concurrency is critical when integrating with systems that cannot scale horizontally as easily as Lambda can. Without concurrency controls, a burst of messages from SQS could cause database saturation, timeouts, and cascading failures. Proper configuration balances throughput with stability and reliability for the entire architecture.",
"key_takeaway": "Use reserved concurrency on a Lambda function to set a hard cap on concurrent executions and protect downstream dependencies from overload.",
"option_explanations": {
"A": "Correct because reserved concurrency directly limits the number of concurrent executions for the Lambda function.",
"B": "Incorrect because memory size changes CPU and performance, not the maximum concurrency.",
"C": "Incorrect because Lambda Destinations route results of asynchronous invocations but do not control concurrency.",
"D": "Incorrect because dead-letter queues capture failed events, not limit parallel processing."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:1",
"service:lambda",
"service:sqs",
"scaling"
]
},
{
"id": "q-d1-lc-002",
"concept_id": "c-lc-throttling",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-1-development",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A Lambda function is invoked synchronously by an API Gateway REST API. During peak traffic, users report intermittent 429 errors. CloudWatch metrics show that the function is hitting its reserved concurrency limit. What is the MOST appropriate action to reduce these errors while preserving protection for a downstream legacy system?",
"options": [
{ "label": "A", "text": "Remove the reserved concurrency limit from the Lambda function." },
{ "label": "B", "text": "Add API Gateway throttling limits that are lower than the Lambda reserved concurrency per second." },
{ "label": "C", "text": "Enable Lambda Destinations for successful invocations." },
{ "label": "D", "text": "Convert the API Gateway integration from synchronous to asynchronous." }
],
"correct_options": ["B"],
"answer_explanation": "API Gateway throttling can be used to smooth traffic before it reaches the Lambda function by limiting requests per second. Setting API Gateway throttling slightly below the Lambda reserved concurrency per-second capacity reduces the chance of exceeding concurrency while still protecting the legacy system. Removing the reserved concurrency removes protection for the downstream system. Lambda Destinations do not affect concurrency or API Gateway 429 errors. Converting to asynchronous would change client semantics and may not be acceptable for synchronous API scenarios.",
"why_this_matters": "Controlling traffic at multiple layers prevents overload and improves user experience. Using API Gateway throttling in tandem with Lambda concurrency controls avoids cascading failures and provides a predictable ceiling on request volume. This helps maintain stability for legacy systems that cannot scale rapidly.",
"key_takeaway": "Use API Gateway throttling together with Lambda reserved concurrency to manage incoming request rates and protect downstream systems.",
"option_explanations": {
"A": "Incorrect because removing reserved concurrency removes downstream protection and can overload the legacy system.",
"B": "Correct because API Gateway throttling smooths traffic and reduces the chance of hitting reserved concurrency limits.",
"C": "Incorrect because Destinations handle results, not request rate or concurrency.",
"D": "Incorrect because switching to asynchronous changes client behavior and does not directly address 429 rate limits."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:1",
"service:lambda",
"service:apigateway",
"throttling"
]
},
{
"id": "q-d1-lc-003",
"concept_id": "c-lc-cold-start-memory",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-1-development",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A developer notices that a Lambda function has long execution times when handling concurrent requests. The function performs CPU-intensive JSON transformations. Which configuration change is MOST likely to improve overall throughput without changing any code?",
"options": [
{ "label": "A", "text": "Increase the function timeout value." },
{ "label": "B", "text": "Increase the function memory size." },
{ "label": "C", "text": "Decrease the function reserved concurrency." },
{ "label": "D", "text": "Disable VPC networking for the function." }
],
"correct_options": ["B"],
"answer_explanation": "Lambda allocates CPU power proportional to the configured memory size. For CPU-intensive processing, increasing the memory size generally increases CPU, reducing execution duration and improving throughput. Increasing the timeout only allows longer runs but does not make them faster. Reducing reserved concurrency might reduce parallelism and lower throughput. Disabling VPC networking affects cold start latency related to ENI creation but does not directly speed up CPU-bound JSON processing once the function is running.",
"why_this_matters": "Right-sizing Lambda memory is a key cost and performance optimization technique. Under-provisioned memory can lead to slow responses and higher overall cost due to longer execution times. Proper configuration helps ensure responsive applications that use resources efficiently.",
"key_takeaway": "For CPU-bound Lambda workloads, increasing memory increases available CPU and can significantly improve execution speed and throughput.",
"option_explanations": {
"A": "Incorrect because a higher timeout lets slow invocations run longer but does not make them faster.",
"B": "Correct because increasing memory also increases CPU, which benefits CPU-intensive processing.",
"C": "Incorrect because lowering reserved concurrency reduces parallelism and likely decreases throughput.",
"D": "Incorrect because VPC networking mainly affects cold starts, not CPU time for JSON transformations."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:1",
"service:lambda",
"performance",
"optimization"
]
},
{
"id": "q-d1-lc-004",
"concept_id": "c-lc-provisioned-concurrency",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-1-development",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "An ecommerce application uses Lambda behind an API Gateway HTTP API. The team observes occasional latency spikes during sudden traffic bursts caused by flash sales. The function uses a Node.js runtime and accesses an RDS database via a VPC. What is the MOST effective way to reduce these latency spikes?",
"options": [
{ "label": "A", "text": "Increase the Lambda function timeout and memory size." },
{ "label": "B", "text": "Configure provisioned concurrency for the Lambda function alias used by production." },
{ "label": "C", "text": "Disable VPC access for the Lambda function." },
{ "label": "D", "text": "Use an SQS queue between API Gateway and Lambda." }
],
"correct_options": ["B"],
"answer_explanation": "Provisioned concurrency keeps a specified number of Lambda execution environments initialized and ready to respond, significantly reducing cold start latency during sudden bursts. Increasing timeout and memory can help performance but will not eliminate cold starts. Disabling VPC access is not an option if the function must reach RDS in a VPC. Introducing SQS between API Gateway and Lambda changes the architecture to asynchronous and may not be suitable for user-facing synchronous requests.",
"why_this_matters": "User-facing APIs must handle unpredictable bursts without degrading user experience. Provisioned concurrency is designed specifically to address cold start issues for latency-sensitive workloads. Proper configuration enhances responsiveness and stability during traffic spikes.",
"key_takeaway": "Use provisioned concurrency on Lambda functions that back latency-sensitive, bursty production traffic to minimize cold start delays.",
"option_explanations": {
"A": "Incorrect because timeout and memory changes do not directly prevent cold starts during bursts.",
"B": "Correct because provisioned concurrency keeps environments warm, reducing cold-start-related latency spikes.",
"C": "Incorrect because the function must access RDS in a VPC and VPC removal is not feasible.",
"D": "Incorrect because inserting SQS would make the path asynchronous, which is not ideal for synchronous API responses."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:1",
"service:lambda",
"service:apigateway",
"provisioned-concurrency"
]
},
{
"id": "q-d1-lc-005",
"concept_id": "c-lc-sqs-batch-size",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-1-development",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A developer configures a Lambda function to process messages from an SQS standard queue. The function is set with a reserved concurrency of 10 and a batch size of 5. How many messages can be processed in parallel at MOST when the queue is heavily loaded?",
"options": [
{ "label": "A", "text": "5 messages, because Lambda processes only one batch at a time." },
{ "label": "B", "text": "10 messages, because reserved concurrency is 10." },
{ "label": "C", "text": "50 messages, because each of the 10 concurrent executions can process a batch of 5 messages." },
{ "label": "D", "text": "Unlimited messages, because SQS scales independently of Lambda concurrency." }
],
"correct_options": ["C"],
"answer_explanation": "With an SQS event source, each concurrent Lambda invocation processes up to the configured batch size. A reserved concurrency of 10 limits the number of concurrent invocations to 10. Each invocation can receive a batch of 5 messages, so up to 50 messages can be processed in parallel. The other options ignore the combination of concurrency and batch size or incorrectly claim unlimited processing.",
"why_this_matters": "Understanding how batch size and concurrency interact is essential for sizing downstream systems and predicting throughput. Misconfiguration can lead to underutilization or overload. Proper calculations help developers design reliable and scalable message processing systems.",
"key_takeaway": "Maximum parallel message processing is approximately reserved concurrency multiplied by batch size for Lambda functions triggered by SQS queues.",
"option_explanations": {
"A": "Incorrect because multiple concurrent Lambda invocations can run in parallel, not just one batch.",
"B": "Incorrect because each of the 10 invocations can process a batch, not just one message.",
"C": "Correct because 10 concurrent invocations with a batch size of 5 results in up to 50 messages in parallel.",
"D": "Incorrect because Lambda concurrency and batch size limit how many messages can be processed simultaneously."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:1",
"service:lambda",
"service:sqs",
"throughput"
]
},
{
"id": "q-d1-lc-006",
"concept_id": "c-lc-reserved-vs-provisioned",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-1-development",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A team wants to ensure that a critical Lambda function always has capacity available even when other functions in the account receive a traffic spike. At the same time, they want to minimize cold start latency for this function during production hours. Which combination of configurations is BEST suited for this requirement?",
"options": [
{ "label": "A", "text": "Use reserved concurrency on the function and enable provisioned concurrency on a production alias." },
{ "label": "B", "text": "Use only provisioned concurrency on the function with no reserved concurrency." },
{ "label": "C", "text": "Use account-level concurrency limits only and no function-level settings." },
{ "label": "D", "text": "Use only reserved concurrency and rely on automatic scaling to reduce cold starts." }
],
"correct_options": ["A"],
"answer_explanation": "Reserved concurrency guarantees a portion of the account's concurrency exclusively for the function, preventing it from being starved by other functions. Provisioned concurrency keeps a specific number of execution environments warm to reduce cold start latency. Using both together satisfies both isolation and low-latency requirements. Using only provisioned concurrency does not protect against account-level concurrency contention. Relying only on account-level limits does not protect the function from other workloads. Reserved concurrency alone does not address cold starts.",
"why_this_matters": "Critical workloads must remain responsive and available even during account-wide spikes. Combining reserved and provisioned concurrency allows teams to guarantee capacity and reduce latency for key services. This improves reliability and user experience during high-load events.",
"key_takeaway": "Combine reserved concurrency for isolation with provisioned concurrency for cold-start reduction on critical Lambda functions.",
"option_explanations": {
"A": "Correct because this combination ensures both guaranteed capacity and reduced cold-start latency.",
"B": "Incorrect because provisioned concurrency alone does not reserve concurrency against other functions' usage.",
"C": "Incorrect because account-level limits do not isolate specific functions from others.",
"D": "Incorrect because reserved concurrency does not eliminate cold starts by itself."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:1",
"service:lambda",
"provisioned-concurrency",
"reserved-concurrency"
]
},
{
"id": "q-d1-lc-007",
"concept_id": "c-lc-throttle-behavior",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A Lambda function is configured with a reserved concurrency of 5. CloudWatch metrics show frequent Throttles for this function when it processes events from an EventBridge rule. What will happen to additional events when the function is throttled?",
"options": [
{ "label": "A", "text": "The events are dropped permanently when throttling occurs." },
{ "label": "B", "text": "EventBridge automatically retries the invocation for a limited period with exponential backoff." },
{ "label": "C", "text": "Lambda automatically queues the events in an internal SQS queue until concurrency becomes available." },
{ "label": "D", "text": "The events are immediately redirected to a dead-letter queue configured on the function." }
],
"correct_options": ["B"],
"answer_explanation": "When EventBridge invokes a Lambda function and receives throttling errors, EventBridge automatically retries the invocation with exponential backoff for a period. Events are not immediately dropped and are not buffered by Lambda in an internal queue. A dead-letter queue for Lambda captures events after retry attempts are exhausted, not immediately. Option B correctly reflects EventBridge retry behavior with throttled Lambda targets.",
"why_this_matters": "Understanding retry behavior is key to designing reliable event-driven architectures and handling backpressure properly. Assuming that events are automatically queued or never retried can cause data loss or unexpected load patterns. Correct expectations help developers choose appropriate DLQ or retry configurations.",
"key_takeaway": "When Lambda is throttled by EventBridge, EventBridge retries the invocation with exponential backoff before optionally sending events to a dead-letter target.",
"option_explanations": {
"A": "Incorrect because EventBridge retries throttled invocations and does not immediately drop events.",
"B": "Correct because EventBridge retries on throttling with exponential backoff for a period.",
"C": "Incorrect because Lambda does not create an internal SQS queue for throttled events.",
"D": "Incorrect because DLQs are used after retries are exhausted, not on the first throttle."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:4",
"service:lambda",
"service:eventbridge",
"retries"
]
},
{
"id": "q-d1-lc-008",
"concept_id": "c-lc-fanout-sns-sqs",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-1-development",
"difficulty_inferred": "hard",
"question_type": "single",
"stem": "A team implements a fanout architecture where an SNS topic notifies three SQS queues, each triggering a separate Lambda function. After a marketing campaign, all three functions experience concurrency spikes, and two downstream databases become overloaded. The team wants to keep the fanout pattern but better manage concurrency. Which solution is the MOST effective and requires the LEAST change to existing integrations?",
"options": [
{ "label": "A", "text": "Reduce the batch size for all SQS event source mappings for the Lambda functions." },
{ "label": "B", "text": "Configure reserved concurrency limits for each Lambda function and adjust SQS visibility timeouts accordingly." },
{ "label": "C", "text": "Replace SNS with EventBridge to reduce the rate of message delivery." },
{ "label": "D", "text": "Use Lambda Destinations to delay processing of SNS messages during spikes." }
],
"correct_options": ["B"],
"answer_explanation": "Reserved concurrency on each Lambda function limits parallel invocations and protects downstream databases. Adjusting SQS visibility timeout ensures that messages are returned to the queue if not processed in time, aligning with the reduced concurrency. Reducing batch size alone affects how many messages each invocation handles but does not guarantee concurrency caps. Replacing SNS with EventBridge does not inherently solve concurrency overload and requires more architectural change. Lambda Destinations do not control when events are processed.",
"why_this_matters": "Fanout architectures can easily overwhelm downstream systems if concurrency is not controlled at each consumer. Per-function concurrency settings and queue timeouts allow teams to manage load without redesigning entire workflows. This leads to more predictable performance during large campaigns or sudden spikes.",
"key_takeaway": "Use reserved concurrency per Lambda consumer and tune SQS timeouts to safely control load in fanout architectures.",
"option_explanations": {
"A": "Incorrect because smaller batches do not inherently cap the number of concurrent Lambda invocations.",
"B": "Correct because reserved concurrency directly limits parallel executions and SQS timeouts align message retries with these limits.",
"C": "Incorrect because switching to EventBridge requires more change and does not automatically limit concurrency.",
"D": "Incorrect because Destinations manage post-processing targets, not concurrency or initial processing timing."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:1",
"service:lambda",
"service:sns",
"service:sqs",
"fanout"
]
},
{
"id": "q-d1-lc-009",
"concept_id": "c-lc-account-limit",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "During a load test, a development team notices that multiple Lambda functions across the account are being throttled even though none of them has a reserved concurrency configured. CloudWatch metrics show that the account's concurrent executions metric is flat at a certain value. What is the MOST likely cause and recommended next step?",
"options": [
{ "label": "A", "text": "The functions reached their maximum memory limit; the team should reduce memory settings." },
{ "label": "B", "text": "The account has reached its configured concurrency limit; the team should request a higher concurrency quota from AWS Support." },
{ "label": "C", "text": "The functions are in a VPC; the team should remove VPC configuration." },
{ "label": "D", "text": "The functions have too many environment variables; the team should reduce environment variables." }
],
"correct_options": ["B"],
"answer_explanation": "When multiple functions without reserved concurrency settings are throttled and the account concurrent executions metric is flat, it indicates the account-level concurrency limit has been reached. The recommended step is to request a quota increase from AWS Support if the load is expected. Memory size, VPC configuration, or number of environment variables do not directly cause account-wide throttling with a flat concurrency metric.",
"why_this_matters": "Understanding the difference between account-level and function-level limits is essential for troubleshooting throttling. Planning capacity and requesting appropriate quotas avoids unexpected throttles in production. This ensures applications remain responsive during legitimate high-load events.",
"key_takeaway": "If Lambda functions across an account are throttled and account concurrent executions are flat, investigate the account concurrency quota and request an increase if needed.",
"option_explanations": {
"A": "Incorrect because memory limits affect cost and performance, not account-level concurrency throttling.",
"B": "Correct because a flat account concurrency metric with throttling indicates the account concurrency quota has been reached.",
"C": "Incorrect because VPC configuration affects cold starts, not global concurrency limits.",
"D": "Incorrect because environment variables do not directly control concurrency or cause throttling."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:4",
"service:lambda",
"limits",
"troubleshooting"
]
},
{
"id": "q-d1-lc-010",
"concept_id": "c-lc-idempotency",
"variant_index": 0,
"topic": "lambda",
"subtopic": "lambda-concurrency",
"domain": "domain-1-development",
"difficulty_inferred": "hard",
"question_type": "single",
"stem": "A Lambda function processes orders from an SQS queue and writes records to a DynamoDB table. Under high concurrency, the team notices occasional duplicate writes when Lambda retries failed batches. They must preserve high concurrency but avoid duplicates. What is the BEST approach?",
"options": [
{ "label": "A", "text": "Reduce the Lambda function's reserved concurrency to 1 so writes are serialized." },
{ "label": "B", "text": "Implement idempotency by using a unique order ID as the DynamoDB partition key and conditional writes." },
{ "label": "C", "text": "Switch the SQS queue to FIFO and rely on exactly-once processing semantics." },
{ "label": "D", "text": "Disable retries for the SQS event source mapping to avoid reprocessing messages." }
],
"correct_options": ["B"],
"answer_explanation": "For high-concurrency systems, idempotency is the recommended approach. Using a unique order ID as the partition key and conditional writes (for example, using a condition expression that the item must not exist) ensures duplicates are rejected while allowing parallel processing. Reducing concurrency to 1 severely limits throughput. FIFO queues provide ordering and limited duplicate suppression but cannot guarantee exactly-once processing. Disabling retries risks losing messages when transient errors occur.",
"why_this_matters": "Distributed, highly concurrent systems inevitably encounter retries and duplicates. Designing idempotent operations allows systems to scale without sacrificing data correctness. This is crucial for financial or order-processing workloads where duplicates are unacceptable.",
"key_takeaway": "Use idempotency with unique identifiers and conditional writes in data stores like DynamoDB to safely handle retries in concurrent Lambda processing.",
"option_explanations": {
"A": "Incorrect because serializing all writes severely reduces scalability and throughput.",
"B": "Correct because idempotent writes with unique keys and condition expressions prevent duplicates while preserving concurrency.",
"C": "Incorrect because FIFO queues do not guarantee exactly-once processing in all failure scenarios.",
"D": "Incorrect because disabling retries may lead to message loss during transient failures."
},
"tags": [
"topic:lambda",
"subtopic:lambda-concurrency",
"domain:1",
"service:lambda",
"service:sqs",
"service:dynamodb",
"idempotency"
]
}
]
}
]
}
]
},
{
"domain_id": "domain-2-security",
"name": "Security",
"topics": [
{
"topic_id": "cognito",
"name": "Amazon Cognito and Application Authentication",
"subtopics": [
{
"subtopic_id": "cognito-auth",
"name": "Cognito authentication and authorization",
"num_questions_generated": 10,
"questions": [
{
"id": "q-d2-ca-001",
"concept_id": "c-ca-user-pool-vs-identity-pool",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A mobile application needs to authenticate users and then provide them with temporary AWS credentials to access an S3 bucket. Which combination of Amazon Cognito features should the developer use?",
"options": [
{ "label": "A", "text": "Cognito user pool only" },
{ "label": "B", "text": "Cognito user pool with a Cognito identity pool" },
{ "label": "C", "text": "Cognito identity pool only with unauthenticated identities" },
{ "label": "D", "text": "An IAM user per mobile user with long-term access keys" }
],
"correct_options": ["B"],
"answer_explanation": "Cognito user pools provide user sign-up and sign-in, returning tokens (ID, access, and refresh). Cognito identity pools use these tokens to federate users and issue temporary AWS credentials via IAM roles. This combination is the recommended way to authenticate application users and grant them scoped AWS access. An identity pool alone with unauthenticated identities does not authenticate users. Creating individual IAM users and distributing long-term credentials to each mobile user is insecure and not scalable.",
"why_this_matters": "Securely granting users limited AWS access is a common requirement for modern applications. Cognito user pools and identity pools together offer a managed way to authenticate users and map them to IAM roles with least-privilege permissions. This avoids embedding long-term credentials in client applications.",
"key_takeaway": "Use Cognito user pools for user authentication and identity pools to exchange tokens for temporary AWS credentials.",
"option_explanations": {
"A": "Incorrect because a user pool alone does not provide AWS credentials.",
"B": "Correct because user pools handle authentication and identity pools issue temporary AWS credentials based on tokens.",
"C": "Incorrect because unauthenticated identities do not validate users and provide anonymous access.",
"D": "Incorrect because IAM users with long-term keys on clients are insecure and hard to manage."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"service:s3",
"authentication"
]
},
{
"id": "q-d2-ca-002",
"concept_id": "c-ca-jwt-validation",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A backend API running on AWS Lambda behind Amazon API Gateway must validate JSON Web Tokens (JWTs) issued by a Cognito user pool. What is the BEST practice for validating these tokens in the Lambda function?",
"options": [
{ "label": "A", "text": "Manually decode the token in the application without verifying the signature." },
{ "label": "B", "text": "Use the JWKS endpoint from the Cognito user pool to validate the token signature and claims." },
{ "label": "C", "text": "Trust any token that includes a valid username claim." },
{ "label": "D", "text": "Disable token verification and rely only on HTTPS to secure the request." }
],
"correct_options": ["B"],
"answer_explanation": "Proper JWT validation includes verifying the token signature against the public keys from the Cognito user pool's JWKS endpoint and checking claims like audience, issuer, and expiration. This ensures that tokens are genuine and intended for the API. Simply decoding the token without signature verification is insecure. Trusting only a username claim or relying solely on HTTPS does not prevent token forgery or misuse.",
"why_this_matters": "Incorrect token validation can allow attackers to forge or reuse tokens and gain unauthorized access. Using the JWKS endpoint ensures that only tokens signed by the expected Cognito user pool are accepted. This is essential for secure microservice and API architectures.",
"key_takeaway": "Always validate JWTs by checking their signature against the identity provider's public keys and by verifying key claims like issuer, audience, and expiration.",
"option_explanations": {
"A": "Incorrect because decoding without verifying the signature does not confirm token authenticity.",
"B": "Correct because using the JWKS endpoint allows proper signature and claim validation.",
"C": "Incorrect because a username claim alone is not sufficient to verify token integrity.",
"D": "Incorrect because HTTPS protects transport, not token integrity or authenticity."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"service:lambda",
"jwt"
]
},
{
"id": "q-d2-ca-003",
"concept_id": "c-ca-app-client-secret",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A web application uses a Cognito user pool and the authorization code grant flow with a confidential client. Where should the application store the OAuth2 client secret used to exchange authorization codes for tokens?",
"options": [
{ "label": "A", "text": "In the browser's local storage." },
{ "label": "B", "text": "In a Lambda function's environment variable encrypted by KMS, accessed from a secure backend." },
{ "label": "C", "text": "Hardcoded in the JavaScript code sent to the client." },
{ "label": "D", "text": "In a public S3 bucket for easy retrieval." }
],
"correct_options": ["B"],
"answer_explanation": "Client secrets for confidential clients must be stored on a secure backend that is not directly exposed to end users. Storing the secret as an encrypted environment variable on a Lambda function and accessing it server-side is a secure pattern. Local storage and hardcoded JavaScript are client-side and can be easily inspected. A public S3 bucket is accessible to anyone and is not suitable for confidential secrets.",
"why_this_matters": "Exposing OAuth client secrets can allow attackers to impersonate the application and obtain tokens fraudulently. Proper secret management is a fundamental security practice and helps maintain trust with identity providers and users.",
"key_takeaway": "Store OAuth client secrets only on secure, server-side components and protect them using mechanisms like KMS-encrypted environment variables or secrets managers.",
"option_explanations": {
"A": "Incorrect because local storage is accessible to end users and potentially malicious scripts.",
"B": "Correct because server-side storage with encryption protects the client secret from exposure.",
"C": "Incorrect because hardcoding secrets in client JavaScript exposes them to all users.",
"D": "Incorrect because a public S3 bucket is world-readable and insecure for secrets."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"service:lambda",
"security"
]
},
{
"id": "q-d2-ca-004",
"concept_id": "c-ca-groups-roles",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "An internal dashboard application uses a Cognito user pool. Users belong to roles such as 'admin' and 'viewer'. The backend API must enforce different levels of access. What is the MOST appropriate way to implement this authorization?",
"options": [
{ "label": "A", "text": "Create separate user pools for admins and viewers." },
{ "label": "B", "text": "Use Cognito user pool groups and include group information in the ID token claims, then implement role-based checks in the API." },
{ "label": "C", "text": "Assign each user an IAM user with policies and authenticate using long-term access keys." },
{ "label": "D", "text": "Use only API Gateway API keys to distinguish between admins and viewers." }
],
"correct_options": ["B"],
"answer_explanation": "Cognito user pool groups allow logical grouping of users and can add group information into ID token claims. The backend can then implement role-based access control by checking these claims. Creating separate user pools increases operational complexity without clear benefits. IAM users with long-term keys are not appropriate for end-user authentication. API keys are meant for metering and throttling, not fine-grained user authorization.",
"why_this_matters": "Fine-grained authorization ensures that only properly authorized users can access sensitive features. Using identity provider claims keeps authorization logic centralized and manageable, reducing risk of privilege escalation.",
"key_takeaway": "Use Cognito user pool groups and token claims for role-based authorization in backend services rather than creating separate user pools or IAM users.",
"option_explanations": {
"A": "Incorrect because multiple user pools complicate management and are unnecessary for simple role separation.",
"B": "Correct because groups and token claims enable straightforward role-based access checks in the API.",
"C": "Incorrect because IAM users with long-term keys are not intended for application end users.",
"D": "Incorrect because API keys are not user identities and do not convey roles."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"rbac",
"authorization"
]
},
{
"id": "q-d2-ca-005",
"concept_id": "c-ca-identity-pool-roles",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A single-page application (SPA) authenticates users with a Cognito user pool and uses a Cognito identity pool to access an S3 bucket. Some users should have read-only access while others should have read/write access. What is the BEST way to configure this behavior?",
"options": [
{ "label": "A", "text": "Create two S3 buckets: one read-only and one read/write, and hardcode different bucket names in the SPA." },
{ "label": "B", "text": "Use identity pool role mappings to assign different IAM roles based on Cognito user pool groups." },
{ "label": "C", "text": "Create a separate identity pool for each user and assign a unique IAM role." },
{ "label": "D", "text": "Give all users full access to S3 and enforce read-only behavior in the client code." }
],
"correct_options": ["B"],
"answer_explanation": "Identity pool role mappings can inspect Cognito user pool group membership and assign different IAM roles accordingly. Each role can grant different S3 permissions, allowing read-only and read/write behavior without duplicating buckets or managing per-user identity pools. Using client-side enforcement alone is insecure, and creating a separate identity pool per user is not scalable.",
"why_this_matters": "Mapping identity provider attributes to IAM roles enables fine-grained, least-privilege access to AWS resources. This reduces the blast radius of compromised credentials and helps meet security and compliance requirements.",
"key_takeaway": "Use Cognito identity pool role mappings with user pool groups to assign different IAM roles and permissions to authenticated users.",
"option_explanations": {
"A": "Incorrect because maintaining multiple buckets and hardcoding names is brittle and unnecessary.",
"B": "Correct because identity pool role mappings based on groups support scalable, least-privilege access.",
"C": "Incorrect because per-user identity pools are unmanageable at scale.",
"D": "Incorrect because relying solely on client-side checks violates least-privilege principles."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"service:s3",
"least-privilege"
]
},
{
"id": "q-d2-ca-006",
"concept_id": "c-ca-token-lifetime",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A developer wants Cognito access tokens to expire after a short period while allowing users to stay signed in to a web application for several hours without re-entering credentials. Which approach BEST meets this requirement?",
"options": [
{ "label": "A", "text": "Set a short expiration for access tokens and a longer expiration for refresh tokens, and use refresh tokens to obtain new access tokens." },
{ "label": "B", "text": "Set long expiration times for both access and ID tokens." },
{ "label": "C", "text": "Disable refresh tokens and rely on automatic reauthentication by Cognito." },
{ "label": "D", "text": "Enable multi-factor authentication to extend token lifetime." }
],
"correct_options": ["A"],
"answer_explanation": "Using short-lived access tokens and longer-lived refresh tokens is a common OAuth2 pattern. The app can use refresh tokens to get new access tokens without user interaction, preserving security while maintaining a good user experience. Long-lived access tokens increase risk if compromised. Disabling refresh tokens forces frequent reauthentication. MFA improves authentication security but does not extend token lifetime.",
"why_this_matters": "Balancing security with usability is critical in authentication design. Short-lived access tokens minimize risk while refresh tokens provide a secure way to maintain sessions. This is a best practice for web and mobile applications.",
"key_takeaway": "Use short-lived access tokens with longer-lived refresh tokens to maintain secure, user-friendly sessions.",
"option_explanations": {
"A": "Correct because this uses refresh tokens to maintain sessions while keeping access tokens short-lived.",
"B": "Incorrect because long-lived access tokens increase the impact of token theft.",
"C": "Incorrect because disabling refresh tokens forces frequent login prompts.",
"D": "Incorrect because MFA affects how users authenticate, not token lifetimes."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"oauth2",
"tokens"
]
},
{
"id": "q-d2-ca-007",
"concept_id": "c-ca-third-party-idp",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "hard",
"question_type": "single",
"stem": "A SaaS application must allow users to sign in using their corporate identities from an external OpenID Connect (OIDC) identity provider while still issuing Cognito user pool tokens that are accepted by existing microservices. How should the developer configure Cognito?",
"options": [
{ "label": "A", "text": "Configure the external OIDC provider as a user pool identity provider and enable federation so Cognito issues its own tokens after successful sign-in." },
{ "label": "B", "text": "Replace the Cognito user pool with the external OIDC provider and update all microservices to validate new tokens." },
{ "label": "C", "text": "Create an identity pool only and disable the user pool." },
{ "label": "D", "text": "Configure SAML federation in IAM and use IAM users for application sign-in." }
],
"correct_options": ["A"],
"answer_explanation": "Cognito user pools support federation with external identity providers, including OIDC. Users authenticate with the external IdP, and Cognito then issues its own tokens, preserving the token format expected by microservices. Replacing Cognito would require changes in all microservices. Identity pools alone do not replace user pools for token issuance. IAM users with SAML federation are not designed for SaaS end-user authentication through Cognito.",
"why_this_matters": "Federating with corporate identity providers lets applications support SSO and central identity management while maintaining existing application token contracts. This reduces integration work and improves security alignment with enterprise identity systems.",
"key_takeaway": "Use Cognito user pool federation with external IdPs so Cognito can issue consistent tokens even when users authenticate with external providers.",
"option_explanations": {
"A": "Correct because OIDC federation into a user pool allows Cognito to issue tokens after external authentication.",
"B": "Incorrect because replacing Cognito requires modifications to all services expecting Cognito tokens.",
"C": "Incorrect because identity pools alone do not provide user pool tokens for microservices.",
"D": "Incorrect because IAM users and SAML federation are not intended for this SaaS user authentication pattern."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"oidc",
"federation"
]
},
{
"id": "q-d2-ca-008",
"concept_id": "c-ca-apigw-authorizer",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "An API Gateway REST API uses a Cognito user pool authorizer to protect its endpoints. A developer wants to pass user identity information to the backend Lambda function. What is the BEST way to achieve this?",
"options": [
{ "label": "A", "text": "Configure API Gateway to forward the JWT token in the Authorization header to the Lambda function." },
{ "label": "B", "text": "Disable the authorizer and read the username from a query string parameter." },
{ "label": "C", "text": "Use API keys to pass the identity of the user to the backend." },
{ "label": "D", "text": "Have the client send the username in a custom header without using tokens." }
],
"correct_options": ["A"],
"answer_explanation": "When using a Cognito user pool authorizer, API Gateway can pass the full JWT token (typically in the Authorization header) to the Lambda function. The function can then validate claims or rely on the authorizer's verified context. Disabling the authorizer or using query parameters or custom headers without tokens is insecure. API keys are not user identities.",
"why_this_matters": "Passing identity information securely allows backend services to apply fine-grained authorization and auditing. Using verified tokens ensures that identity data is trustworthy and not forged by the client.",
"key_takeaway": "Forward the verified JWT from API Gateway to backend services so they can rely on token claims for authorization and auditing.",
"option_explanations": {
"A": "Correct because forwarding the JWT token gives the backend access to secure identity claims.",
"B": "Incorrect because removing the authorizer and using query parameters is insecure.",
"C": "Incorrect because API keys do not represent individual users.",
"D": "Incorrect because sending usernames without tokens can be easily spoofed."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"service:apigateway",
"authorization"
]
},
{
"id": "q-d2-ca-009",
"concept_id": "c-ca-microservice-claims",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "hard",
"question_type": "single",
"stem": "A microservices architecture uses Cognito user pool tokens for user identity. An API gateway service receives the token and calls several downstream services. To avoid each service validating the token independently, the team wants a simple way to propagate trusted user context. What is the BEST practice?",
"options": [
{ "label": "A", "text": "Have the API gateway service validate the token once and pass a signed, minimal identity context (such as a JWT or headers) to downstream services." },
{ "label": "B", "text": "Have every microservice re-authenticate the user directly against Cognito." },
{ "label": "C", "text": "Strip identity information at the gateway and let each service treat the user as anonymous." },
{ "label": "D", "text": "Use API Gateway API keys as the primary identity mechanism for internal calls." }
],
"correct_options": ["A"],
"answer_explanation": "Validating tokens at a single entry point (such as an API gateway service) and then propagating a signed, minimal identity context is a common pattern that reduces complexity while maintaining trust. Each downstream service can trust the signed context from the gateway. Having every service re-authenticate against Cognito increases latency and complexity. Stripping identity information removes the ability to enforce user-level authorization. API keys are not suitable as the primary identity for internal user-level authorization.",
"why_this_matters": "Centralized authentication with distributed authorization allows large systems to scale without duplicating complex token validation logic. This improves performance and reduces the risk of inconsistent security checks.",
"key_takeaway": "Validate user tokens at the system boundary and propagate a trusted, signed identity context to downstream services.",
"option_explanations": {
"A": "Correct because centralized validation with a signed context is a scalable, secure pattern.",
"B": "Incorrect because re-authenticating at each service adds latency and complexity.",
"C": "Incorrect because it removes user context needed for authorization.",
"D": "Incorrect because API keys are not a replacement for authenticated user identities."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"microservices",
"security"
]
},
{
"id": "q-d2-ca-010",
"concept_id": "c-ca-least-privilege",
"variant_index": 0,
"topic": "cognito",
"subtopic": "cognito-auth",
"domain": "domain-2-security",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A developer configures a Cognito identity pool to allow authenticated users to download files from a private S3 bucket. To follow least-privilege principles, which IAM policy is MOST appropriate to attach to the role for authenticated identities?",
"options": [
{ "label": "A", "text": "An S3 policy that allows s3:GetObject only on the specific bucket and prefix used by the application." },
{ "label": "B", "text": "An S3 policy that allows s3:* on all buckets in the account." },
{ "label": "C", "text": "An IAM policy that allows all actions on all services." },
{ "label": "D", "text": "No policy, because Cognito automatically grants access to S3 for authenticated users." }
],
"correct_options": ["A"],
"answer_explanation": "Least privilege means granting only the permissions needed for the role's tasks. For downloading files, users typically need s3:GetObject access to a specific bucket and optional prefix. Granting s3:* on all buckets or all actions on all services is overly permissive. Cognito does not automatically grant access to S3; it relies on attached IAM policies.",
"why_this_matters": "Overly permissive IAM policies increase the impact of compromised credentials and misconfigurations. Scoping resource-level permissions helps control risk and meet compliance requirements.",
"key_takeaway": "Attach narrowly scoped IAM policies to Cognito roles, granting only the specific S3 actions and resources required.",
"option_explanations": {
"A": "Correct because it grants only necessary s3:GetObject access to specific resources.",
"B": "Incorrect because s3:* on all buckets is overly broad.",
"C": "Incorrect because allowing all actions on all services violates least privilege.",
"D": "Incorrect because Cognito does not automatically provide S3 access without IAM policies."
},
"tags": [
"topic:cognito",
"subtopic:cognito-auth",
"domain:2",
"service:cognito",
"service:s3",
"least-privilege"
]
}
]
}
]
}
]
},
{
"domain_id": "domain-3-deployment",
"name": "Deployment",
"topics": [
{
"topic_id": "ci-cd",
"name": "CI/CD and Application Deployment",
"subtopics": [
{
"subtopic_id": "ci-cd-with-codepipeline",
"name": "Continuous integration and deployment with CodePipeline",
"num_questions_generated": 10,
"questions": [
{
"id": "q-d3-cp-001",
"concept_id": "c-cp-basic-flow",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-3-deployment",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A team wants to automatically build, test, and deploy a Lambda-based application whenever code is pushed to a main branch in CodeCommit. Which AWS service should they use to orchestrate this end-to-end CI/CD workflow?",
"options": [
{ "label": "A", "text": "AWS CodeBuild" },
{ "label": "B", "text": "AWS CodePipeline" },
{ "label": "C", "text": "AWS CloudFormation" },
{ "label": "D", "text": "Amazon EventBridge" }
],
"correct_options": ["B"],
"answer_explanation": "AWS CodePipeline is a fully managed continuous delivery service that orchestrates stages such as source, build, test, and deploy. CodeBuild is used within a pipeline as a build provider, not as the orchestrator itself. CloudFormation provisions infrastructure but does not coordinate full CI/CD workflows by itself. EventBridge can trigger pipelines or actions but is not a CI/CD orchestrator.",
"why_this_matters": "An orchestrated pipeline reduces manual steps, speeds up deployments, and enforces consistent release processes. Using the right service as the pipeline backbone is key to building reliable and auditable delivery workflows.",
"key_takeaway": "Use CodePipeline to orchestrate multi-stage CI/CD workflows and integrate services like CodeCommit, CodeBuild, and deployment targets.",
"option_explanations": {
"A": "Incorrect because CodeBuild performs builds but does not orchestrate the entire pipeline.",
"B": "Correct because CodePipeline coordinates source, build, test, and deploy stages.",
"C": "Incorrect because CloudFormation provisions resources but is not a CI/CD orchestrator.",
"D": "Incorrect because EventBridge is used for event routing, not full CI/CD orchestration."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:3",
"service:codepipeline",
"deployment"
]
},
{
"id": "q-d3-cp-002",
"concept_id": "c-cp-manual-approval",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-3-deployment",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A company requires a security review before deploying changes to the production environment. The development team uses CodePipeline with build and test stages already defined. What is the MOST appropriate way to enforce this review before production deployment?",
"options": [
{ "label": "A", "text": "Add a manual approval action between the test stage and the production deploy stage in CodePipeline." },
{ "label": "B", "text": "Require developers to send an email before merging to the main branch." },
{ "label": "C", "text": "Store a checklist in S3 and ask developers to confirm it manually." },
{ "label": "D", "text": "Add a second build action that compiles security documentation." }
],
"correct_options": ["A"],
"answer_explanation": "CodePipeline supports manual approval actions that pause the pipeline until an authorized reviewer approves or rejects the deployment. Placing this between the test and production stages enforces human review. Emails or checklists outside the pipeline are not enforced controls. An extra build action does not ensure that security has approved the release.",
"why_this_matters": "Regulated or security-sensitive environments often require human approval before production changes. Integrating approvals into the pipeline ensures that governance is enforced and auditable.",
"key_takeaway": "Use CodePipeline manual approval actions to enforce human reviews at key points in the deployment workflow.",
"option_explanations": {
"A": "Correct because manual approval actions are built into CodePipeline for this purpose.",
"B": "Incorrect because email-based processes are not enforced by the pipeline.",
"C": "Incorrect because informal checklists do not enforce or record approvals.",
"D": "Incorrect because another build does not involve a human security review."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:3",
"service:codepipeline",
"governance"
]
},
{
"id": "q-d3-cp-003",
"concept_id": "c-cp-artifacts-s3",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-3-deployment",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A pipeline builds a container image using CodeBuild and then deploys it to Amazon ECS. The team wants to store build artifacts such as test reports and configuration files for later inspection. Where should these artifacts be stored for best integration with CodePipeline?",
"options": [
{ "label": "A", "text": "An S3 bucket configured as a CodePipeline artifact store." },
{ "label": "B", "text": "An EBS volume attached to the CodeBuild instance." },
{ "label": "C", "text": "A local folder on the developer's laptop." },
{ "label": "D", "text": "A DynamoDB table storing the files in binary attributes." }
],
"correct_options": ["A"],
"answer_explanation": "CodePipeline uses S3 buckets as artifact stores for pipeline artifacts such as build outputs, templates, and reports. CodeBuild can output artifacts directly to this S3 location. EBS volumes are ephemeral for CodeBuild environments and not directly integrated as pipeline artifact stores. Developer laptops and DynamoDB are not suitable or standard for storing pipeline artifacts.",
"why_this_matters": "Centralized artifact storage provides traceability and debugging capabilities for builds and deployments. Using the native artifact store integration simplifies configuration and permissions.",
"key_takeaway": "Configure an S3 bucket as the CodePipeline artifact store and direct CodeBuild artifacts there for consistent storage and retrieval.",
"option_explanations": {
"A": "Correct because S3 artifact stores are the standard destination for CodePipeline artifacts.",
"B": "Incorrect because EBS volumes used by CodeBuild are temporary and not managed as pipeline artifact stores.",
"C": "Incorrect because local storage on developers' laptops is not integrated or reliable.",
"D": "Incorrect because DynamoDB is not designed for storing build artifact files."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:3",
"service:codepipeline",
"service:codebuild",
"service:s3"
]
},
{
"id": "q-d3-cp-004",
"concept_id": "c-cp-multi-env",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-3-deployment",
"difficulty_inferred": "hard",
"question_type": "single",
"stem": "A development team uses a single CodePipeline pipeline to deploy a serverless application to dev, test, and prod environments. Each environment must use different configuration values such as API throttling limits and feature flags. What is the BEST way to manage these differences while keeping the deployment artifact the same across environments?",
"options": [
{ "label": "A", "text": "Bake environment-specific values directly into the Lambda function code at build time." },
{ "label": "B", "text": "Store configuration in AWS AppConfig or Parameter Store and reference environment-specific parameters during deployment." },
{ "label": "C", "text": "Create separate pipelines with separate code repositories for each environment." },
{ "label": "D", "text": "Use different branches for each environment and change code constants before merging." }
],
"correct_options": ["B"],
"answer_explanation": "Storing configuration externally in services like AWS AppConfig or Systems Manager Parameter Store allows the same code artifact to be deployed to each environment, with the environment selecting appropriate configuration at deploy or runtime. Baking config into code or using separate repos or branches reduces consistency and increases operational overhead. Externalizing configuration aligns with twelve-factor app principles.",
"why_this_matters": "Separating configuration from code reduces drift between environments and simplifies deployments. It enables safer rollouts and easier changes to configuration without rebuilding or redeploying application binaries.",
"key_takeaway": "Use external configuration services to manage environment-specific settings while reusing the same deployment artifact.",
"option_explanations": {
"A": "Incorrect because embedding config in code couples deployments to config changes.",
"B": "Correct because external config services enable one artifact with environment-specific configurations.",
"C": "Incorrect because multiple pipelines and repos increase complexity and risk drift.",
"D": "Incorrect because per-branch code constants are error-prone and complicate version control."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:3",
"service:codepipeline",
"service:ssm",
"service:appconfig"
]
},
{
"id": "q-d3-cp-005",
"concept_id": "c-cp-failed-action",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A CodePipeline execution fails at a CodeBuild test action. The team wants to be notified immediately and see detailed failure logs. What is the MOST efficient way to accomplish this?",
"options": [
{ "label": "A", "text": "Enable CloudWatch Events (EventBridge) for CodePipeline state changes and send failure notifications to an SNS topic." },
{ "label": "B", "text": "Manually check the CodeBuild console once a day." },
{ "label": "C", "text": "Write a custom script that polls the CodePipeline API for failures every hour." },
{ "label": "D", "text": "Rerun the pipeline without investigating logs, assuming a transient error." }
],
"correct_options": ["A"],
"answer_explanation": "CodePipeline emits state change events that can be captured via EventBridge and routed to SNS for notifications. From the notification, the team can navigate to detailed CodeBuild logs in CloudWatch Logs. Manual checks or polling scripts are inefficient and error-prone. Rerunning without inspecting logs misses root cause analysis.",
"why_this_matters": "Fast feedback on build and test failures reduces time-to-fix and improves deployment quality. Integrating notifications and logs with pipeline events is key to an efficient DevOps workflow.",
"key_takeaway": "Use EventBridge and SNS to subscribe to CodePipeline state change events and quickly investigate build logs when failures occur.",
"option_explanations": {
"A": "Correct because EventBridge with SNS provides near-real-time notifications for failures.",
"B": "Incorrect because daily manual checks delay response to failures.",
"C": "Incorrect because custom polling is unnecessary and less reliable than events.",
"D": "Incorrect because rerunning without investigation ignores underlying issues."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:4",
"service:codepipeline",
"service:codebuild",
"service:eventbridge"
]
},
{
"id": "q-d3-cp-006",
"concept_id": "c-cp-sam-deploy",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-3-deployment",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A serverless application is defined using an AWS SAM template. The team wants to deploy it automatically via CodePipeline. Which step is REQUIRED in the pipeline to prepare the SAM template for deployment with CloudFormation?",
"options": [
{ "label": "A", "text": "Run sam package or sam build to transform the SAM template into a CloudFormation template and upload artifacts to S3." },
{ "label": "B", "text": "Manually upload Lambda code zips to each region." },
{ "label": "C", "text": "Convert the SAM template into a Dockerfile." },
{ "label": "D", "text": "Replace SAM resources with JSON Policy documents." }
],
"correct_options": ["A"],
"answer_explanation": "SAM templates must be transformed and packaged (via sam package or sam build/deploy) to upload code artifacts to S3 and produce a CloudFormation-compatible template. This step can be performed in a CodeBuild action within the pipeline. Manual uploads, Dockerfiles, or policy document modifications are unrelated to preparing the SAM template for CloudFormation deployment.",
"why_this_matters": "Automating the SAM build and packaging process keeps infrastructure as code reproducible across environments. It also ensures all artifacts are versioned and properly referenced in CloudFormation stacks.",
"key_takeaway": "Include a SAM packaging/build step in your CI/CD pipeline to generate deployable CloudFormation templates and upload artifacts.",
"option_explanations": {
"A": "Correct because SAM packaging transforms the template and uploads artifacts for CloudFormation.",
"B": "Incorrect because manual uploads break automation and traceability.",
"C": "Incorrect because SAM templates are not converted to Dockerfiles for standard deployments.",
"D": "Incorrect because policy documents are unrelated to SAM template transformation."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:3",
"service:codepipeline",
"service:cloudformation",
"service:sam"
]
},
{
"id": "q-d3-cp-007",
"concept_id": "c-cp-blue-green",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-3-deployment",
"difficulty_inferred": "hard",
"question_type": "single",
"stem": "A team deploys an ECS service behind an Application Load Balancer (ALB) using CodePipeline and CodeDeploy. They require the ability to shift a small percentage of traffic to a new task set, monitor it, and then shift all traffic if healthy. Which deployment configuration should they choose in CodeDeploy?",
"options": [
{ "label": "A", "text": "AllAtOnce" },
{ "label": "B", "text": "Linear deployment with equal increments" },
{ "label": "C", "text": "Canary deployment configuration" },
{ "label": "D", "text": "In-place deployment without a load balancer" }
],
"correct_options": ["C"],
"answer_explanation": "Canary deployment configurations in CodeDeploy shift a small percentage of traffic to the new version, wait for a specified interval, then shift the remaining traffic if health checks pass. AllAtOnce immediately shifts all traffic. Linear shifts fixed increments over time but may not be as targeted as a canary. In-place deployments without a load balancer do not support controlled traffic shifting.",
"why_this_matters": "Progressive delivery techniques like canary releases reduce risk by limiting the impact of faulty deployments. Integration with load balancers and deployment controllers allows automated rollback based on health checks.",
"key_takeaway": "Use CodeDeploy canary configurations for ECS or Lambda when you need controlled traffic shifting and health-based promotion.",
"option_explanations": {
"A": "Incorrect because AllAtOnce shifts all traffic at once, increasing risk.",
"B": "Incorrect because linear deployment shifts traffic in equal increments, not a small canary slice followed by the remainder.",
"C": "Correct because canary deployment starts with a small percentage of traffic before promotion.",
"D": "Incorrect because an in-place deployment without a load balancer does not support traffic shifting."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:3",
"service:codedeploy",
"service:ecs",
"deployment-strategy"
]
},
{
"id": "q-d3-cp-008",
"concept_id": "c-cp-source-triggers",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-3-deployment",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A developer wants the pipeline to start automatically when new code is pushed to a specific branch in a GitHub repository. How can this be configured in CodePipeline?",
"options": [
{ "label": "A", "text": "Configure a GitHub source action in CodePipeline with a webhook on the desired branch." },
{ "label": "B", "text": "Manually start the pipeline after each push." },
{ "label": "C", "text": "Schedule the pipeline to run once per day using CloudWatch Events." },
{ "label": "D", "text": "Use an SQS queue to poll the repository for changes." }
],
"correct_options": ["A"],
"answer_explanation": "CodePipeline supports GitHub as a source provider and can be configured with a webhook so that pushes to a specific branch automatically trigger pipeline executions. Manual starts, daily schedules, or SQS-based polling are less efficient and do not respond immediately to changes.",
"why_this_matters": "Automated triggers keep CI/CD pipelines responsive and reduce manual work, improving developer productivity and ensuring that code changes are quickly validated and deployed.",
"key_takeaway": "Use native source integrations like GitHub webhooks to automatically trigger CodePipeline executions on code changes.",
"option_explanations": {
"A": "Correct because GitHub source actions with webhooks integrate directly with CodePipeline.",
"B": "Incorrect because manual starts are error-prone and not continuous integration.",
"C": "Incorrect because scheduled runs are not immediate and may delay feedback.",
"D": "Incorrect because polling with SQS is not a standard pattern for Git repos."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:3",
"service:codepipeline",
"integration"
]
},
{
"id": "q-d3-cp-009",
"concept_id": "c-cp-cross-account",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-3-deployment",
"difficulty_inferred": "hard",
"question_type": "single",
"stem": "A central CI/CD account runs CodePipeline that must deploy CloudFormation stacks into multiple application accounts. What is the MOST secure way to grant deployment permissions to the pipeline?",
"options": [
{ "label": "A", "text": "Create IAM users in each application account and store their access keys as plaintext environment variables in CodeBuild." },
{ "label": "B", "text": "Use cross-account IAM roles in each application account and have the pipeline assume these roles via a CloudFormation or CodeBuild action." },
{ "label": "C", "text": "Enable root access in each application account and share the root credentials with the CI/CD account." },
{ "label": "D", "text": "Copy CloudFormation templates manually to each application account and deploy them by hand." }
],
"correct_options": ["B"],
"answer_explanation": "Using cross-account IAM roles and having the pipeline assume them is the recommended secure pattern for cross-account deployments. It avoids long-term credentials and limits permissions to the needed scope. IAM users with stored access keys, root credentials, and manual deployments are insecure, unscalable, or both.",
"why_this_matters": "Centralized CI/CD across multiple accounts improves governance, but it must be implemented securely. Role assumption protects against credential leakage and enforces least privilege across accounts.",
"key_takeaway": "Use cross-account IAM roles and role assumption from CodePipeline or CodeBuild for secure multi-account deployments.",
"option_explanations": {
"A": "Incorrect because long-term access keys in environment variables are insecure and hard to rotate.",
"B": "Correct because cross-account roles with AssumeRole are the standard secure pattern.",
"C": "Incorrect because sharing root credentials is a severe security risk.",
"D": "Incorrect because manual deployment does not scale and is error-prone."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:3",
"service:codepipeline",
"service:iam",
"cross-account"
]
},
{
"id": "q-d3-cp-010",
"concept_id": "c-cp-rollback",
"variant_index": 0,
"topic": "ci-cd",
"subtopic": "ci-cd-with-codepipeline",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "After a new version of a Lambda function is deployed via CodePipeline and CodeDeploy, monitoring shows increased error rates. The team wants to quickly rollback to the previous version. What is the MOST efficient approach?",
"options": [
{ "label": "A", "text": "Manually upload the previous Lambda deployment package using the console." },
{ "label": "B", "text": "Use CodeDeploy to rollback the deployment to the previous Lambda function version and alias." },
{ "label": "C", "text": "Terminate the Lambda function and recreate it from scratch." },
{ "label": "D", "text": "Wait for errors to subside, assuming they are transient." }
],
"correct_options": ["B"],
"answer_explanation": "When using CodeDeploy with Lambda, the service keeps track of versions and can rollback deployments based on alarms or manual triggers, updating the alias to point to the previous version. Manually uploading packages or recreating the function increases risk and effort. Ignoring sustained errors can damage reliability and user trust.",
"why_this_matters": "Automated or quick rollbacks limit the impact of faulty deployments, improving system resilience and reducing downtime. Integrating rollback with CI/CD is a key DevOps practice.",
"key_takeaway": "Leverage CodeDeploy's rollback capabilities with Lambda aliases to quickly revert to known-good versions when issues arise.",
"option_explanations": {
"A": "Incorrect because manual uploads are slow and error-prone.",
"B": "Correct because CodeDeploy supports automated and manual rollbacks for Lambda deployments.",
"C": "Incorrect because recreating the function is unnecessary and disruptive.",
"D": "Incorrect because ignoring persistent errors undermines reliability."
},
"tags": [
"topic:ci-cd",
"subtopic:ci-cd-with-codepipeline",
"domain:4",
"service:codedeploy",
"service:lambda",
"rollback"
]
}
]
}
]
}
]
},
{
"domain_id": "domain-4-troubleshooting-optimization",
"name": "Troubleshooting and Optimization",
"topics": [
{
"topic_id": "observability",
"name": "Logging, Metrics, and Tracing",
"subtopics": [
{
"subtopic_id": "observability-with-cloudwatch-and-xray",
"name": "Observability with CloudWatch and X-Ray",
"num_questions_generated": 10,
"questions": [
{
"id": "q-d4-ox-001",
"concept_id": "c-ox-structured-logging",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A Lambda function writes plain text logs to CloudWatch Logs. The team finds it difficult to search for specific fields such as requestId and userId. What should the developer do to make logs easier to query?",
"options": [
{ "label": "A", "text": "Use structured logging with a consistent JSON format for log messages." },
{ "label": "B", "text": "Reduce logging to only error messages." },
{ "label": "C", "text": "Disable logging to improve performance." },
{ "label": "D", "text": "Store logs in local files instead of CloudWatch Logs." }
],
"correct_options": ["A"],
"answer_explanation": "Structured logging using JSON allows CloudWatch Logs and CloudWatch Logs Insights to parse fields, making it easy to filter and aggregate by attributes like requestId and userId. Reducing or disabling logging makes troubleshooting harder. Local files are not centrally collected or searchable in CloudWatch.",
"why_this_matters": "Good observability depends on logs that are easy to search, filter, and analyze at scale. Structured logs improve visibility and reduce time to troubleshoot complex issues.",
"key_takeaway": "Use structured JSON logging to make CloudWatch Logs easier to query and analyze with Logs Insights.",
"option_explanations": {
"A": "Correct because structured JSON logging enables field-based queries.",
"B": "Incorrect because logging only errors reduces available diagnostic information.",
"C": "Incorrect because disabling logging removes crucial troubleshooting data.",
"D": "Incorrect because local log files are not centralized or searchable across instances."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:cloudwatch",
"logging"
]
},
{
"id": "q-d4-ox-002",
"concept_id": "c-ox-xray-tracing",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A microservices application uses API Gateway, Lambda, and DynamoDB. The team wants to identify which part of the request path is causing high latency. Which AWS service should they enable to get end-to-end traces with segment-level timing?",
"options": [
{ "label": "A", "text": "Amazon CloudWatch Logs" },
{ "label": "B", "text": "AWS X-Ray" },
{ "label": "C", "text": "Amazon CloudWatch Dashboards" },
{ "label": "D", "text": "AWS CloudTrail" }
],
"correct_options": ["B"],
"answer_explanation": "AWS X-Ray provides distributed tracing and visualizes end-to-end requests, showing segment-level latency across services such as API Gateway, Lambda, and DynamoDB. CloudWatch Logs and Dashboards show logs and metrics but do not provide full trace visualizations. CloudTrail records API calls for auditing, not performance tracing.",
"why_this_matters": "Distributed tracing is essential for understanding performance bottlenecks in microservice architectures where a single request can involve many components. X-Ray reduces the effort needed to pinpoint latency hotspots.",
"key_takeaway": "Use AWS X-Ray to trace requests across services and identify where latency occurs in distributed applications.",
"option_explanations": {
"A": "Incorrect because CloudWatch Logs provide log events but not full distributed tracing.",
"B": "Correct because X-Ray is designed for end-to-end distributed tracing and latency analysis.",
"C": "Incorrect because Dashboards visualize metrics, not traces.",
"D": "Incorrect because CloudTrail focuses on audit logs for API calls."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:xray",
"service:cloudwatch",
"tracing"
]
},
{
"id": "q-d4-ox-003",
"concept_id": "c-ox-cw-metrics-alarms",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A web API must alert the on-call developer when the 5XX error rate exceeds 5% over a 5-minute period. Which combination of CloudWatch features should be used?",
"options": [
{ "label": "A", "text": "CloudWatch metric for 5XX error count and a CloudWatch alarm that publishes to an SNS topic." },
{ "label": "B", "text": "CloudWatch Logs only, without metrics or alarms." },
{ "label": "C", "text": "CloudTrail event history with no alarms." },
{ "label": "D", "text": "CloudWatch Dashboards with manual monitoring." }
],
"correct_options": ["A"],
"answer_explanation": "CloudWatch metrics track API Gateway 5XX error counts and rates. A CloudWatch alarm can evaluate the metric over a 5-minute period and publish notifications to SNS, which can send email or messages to on-call systems. Logs, CloudTrail, or dashboards without alarms do not provide automated notifications.",
"why_this_matters": "Automated alerts help teams respond quickly to incidents and reduce downtime. Without alarms, teams may not notice issues promptly, impacting reliability and user experience.",
"key_takeaway": "Use CloudWatch metrics and alarms with SNS to automatically notify teams when error rates exceed defined thresholds.",
"option_explanations": {
"A": "Correct because metrics plus alarms and SNS provide automated monitoring and notifications.",
"B": "Incorrect because logs alone do not generate proactive alerts.",
"C": "Incorrect because CloudTrail is for audit logging, not operational metrics.",
"D": "Incorrect because manual dashboard monitoring is unreliable."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:cloudwatch",
"service:sns",
"alarms"
]
},
{
"id": "q-d4-ox-004",
"concept_id": "c-ox-emf-custom-metrics",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A Lambda function needs to emit custom business metrics such as number of orders processed and total order value without making additional network calls. What is the MOST efficient way to send these metrics to CloudWatch?",
"options": [
{ "label": "A", "text": "Use CloudWatch Embedded Metric Format (EMF) in structured logs and configure CloudWatch Logs to extract metrics." },
{ "label": "B", "text": "Call the PutMetricData API synchronously on every request." },
{ "label": "C", "text": "Write metrics to a local file and upload it daily." },
{ "label": "D", "text": "Send metrics via email to an SNS topic." }
],
"correct_options": ["A"],
"answer_explanation": "CloudWatch Embedded Metric Format allows applications to embed metric data in log events, which CloudWatch automatically extracts into metrics without extra network calls. PutMetricData works but adds direct API calls per invocation, which may increase latency and cost. Local files and email are not appropriate methods for metrics ingestion.",
"why_this_matters": "Efficient metrics emission is important for performance and cost, especially in high-throughput environments like Lambda. EMF simplifies emitting custom metrics while keeping overhead low.",
"key_takeaway": "Use CloudWatch EMF in structured logs from Lambda to generate custom metrics without separate API calls.",
"option_explanations": {
"A": "Correct because EMF embeds metrics in logs for automatic ingestion.",
"B": "Incorrect because calling PutMetricData for every request adds unnecessary overhead.",
"C": "Incorrect because local files require manual processing and are not real-time.",
"D": "Incorrect because email is not a metrics ingestion mechanism."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:cloudwatch",
"metrics",
"emf"
]
},
{
"id": "q-d4-ox-005",
"concept_id": "c-ox-correlation-id",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A distributed system consists of API Gateway, multiple Lambda functions, and an SQS queue. Debugging an issue across components is difficult because logs cannot be easily tied to a single user request. What should the developer implement to improve traceability?",
"options": [
{ "label": "A", "text": "A correlation ID that is generated at the entry point and propagated through all calls and log entries." },
{ "label": "B", "text": "A new CloudFormation stack for each microservice." },
{ "label": "C", "text": "A separate S3 bucket to store all logs as raw text files." },
{ "label": "D", "text": "Random log message prefixes for each service." }
],
"correct_options": ["A"],
"answer_explanation": "Using a correlation ID generated at the system entry point and passing it through headers, messages, and log entries allows developers to group logs and traces for a single request. CloudFormation stacks, S3 storage, or random prefixes do not systematically tie logs across services to the same request.",
"why_this_matters": "Correlation IDs are fundamental to observability in distributed systems, making it possible to follow a request through multiple components and quickly identify where issues occur.",
"key_takeaway": "Implement a correlation ID that follows each request through all services and logs to simplify cross-service debugging.",
"option_explanations": {
"A": "Correct because correlation IDs provide a consistent identifier to tie logs together.",
"B": "Incorrect because CloudFormation stacks are deployment units, not trace identifiers.",
"C": "Incorrect because S3 storage alone does not provide cross-service correlation.",
"D": "Incorrect because random prefixes do not guarantee consistency across a single request."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:cloudwatch",
"correlation-id",
"logging"
]
},
{
"id": "q-d4-ox-006",
"concept_id": "c-ox-log-retention",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "CloudWatch Logs costs are increasing due to long-term storage of application logs that are rarely accessed after 30 days. What is the MOST operationally efficient way to reduce log storage cost while retaining recent logs?",
"options": [
{ "label": "A", "text": "Set a 30-day retention policy on the relevant CloudWatch log groups." },
{ "label": "B", "text": "Disable logging in production environments." },
{ "label": "C", "text": "Download all logs to local storage and delete them from AWS." },
{ "label": "D", "text": "Set a retention policy of 'Never Expire' for all log groups." }
],
"correct_options": ["A"],
"answer_explanation": "CloudWatch log group retention settings allow automatic deletion of logs older than a specified period, such as 30 days. This retains recent logs needed for troubleshooting while controlling storage costs. Disabling logging removes essential diagnostic data. Downloading logs manually is operationally heavy. 'Never Expire' offers no cost control for long-lived logs.",
"why_this_matters": "Cost management is an important aspect of observability. Retention policies ensure that logs remain useful for troubleshooting without accumulating unnecessary long-term storage costs.",
"key_takeaway": "Configure CloudWatch Logs retention policies to automatically delete old logs that are no longer needed.",
"option_explanations": {
"A": "Correct because log retention policies reduce storage costs automatically after a defined period.",
"B": "Incorrect because disabling logging severely limits troubleshooting.",
"C": "Incorrect because manual log management is inefficient and fragile.",
"D": "Incorrect because never expiring logs leads to unbounded cost growth."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:cloudwatch",
"cost-optimization"
]
},
{
"id": "q-d4-ox-007",
"concept_id": "c-ox-xray-sampling",
"variant_index": 0,
"topic": "observability",
"subtopic:": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A high-traffic production API has X-Ray tracing enabled for all requests, causing increased overhead and cost. The team still needs visibility into performance issues. What is the BEST way to reduce overhead while preserving useful tracing data?",
"options": [
{ "label": "A", "text": "Disable X-Ray entirely in production." },
{ "label": "B", "text": "Configure X-Ray sampling rules to trace only a subset of requests and all requests that result in errors." },
{ "label": "C", "text": "Enable X-Ray only on weekends." },
{ "label": "D", "text": "Use X-Ray only for health checks from the load balancer." }
],
"correct_options": ["B"],
"answer_explanation": "X-Ray sampling rules allow applications to trace a subset of requests, reducing overhead and cost while still providing statistically representative data, and can be configured to always trace errors. Disabling X-Ray removes observability. Limiting tracing to weekends or health checks does not provide representative traces during normal usage.",
"why_this_matters": "Sampling balances observability and cost in high-traffic systems. It ensures developers have enough data to identify issues without tracing every single request.",
"key_takeaway": "Use X-Ray sampling rules to trace representative traffic and all error cases instead of tracing 100% of requests.",
"option_explanations": {
"A": "Incorrect because disabling X-Ray removes critical tracing data.",
"B": "Correct because sampling rules reduce overhead while preserving valuable tracing information.",
"C": "Incorrect because tracing only on weekends misses most production traffic patterns.",
"D": "Incorrect because health checks are not representative of real user requests."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:xray",
"sampling",
"cost-optimization"
]
},
{
"id": "q-d4-ox-008",
"concept_id": "c-ox-log-filter-patterns",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "medium",
"question_type": "single",
"stem": "A team wants to trigger an alert whenever the text 'PAYMENT_FAILED' appears in application logs. Which CloudWatch feature should they use?",
"options": [
{ "label": "A", "text": "CloudWatch Logs metric filter with an alarm on the resulting metric." },
{ "label": "B", "text": "CloudWatch Dashboards to manually view logs." },
{ "label": "C", "text": "CloudTrail to monitor all API calls." },
{ "label": "D", "text": "X-Ray traces for all requests." }
],
"correct_options": ["A"],
"answer_explanation": "CloudWatch Logs metric filters can search for specific text patterns such as 'PAYMENT_FAILED' in log events and increment a metric when a match is found. A CloudWatch alarm on this metric can then notify the team. Dashboards, CloudTrail, and X-Ray do not directly implement text-based log pattern alerts.",
"why_this_matters": "Detecting specific error events in logs and turning them into actionable alerts helps teams respond quickly to critical business failures, such as payment issues.",
"key_takeaway": "Use CloudWatch Logs metric filters to detect log patterns and trigger alarms for important events.",
"option_explanations": {
"A": "Correct because metric filters convert log pattern matches into metrics that can trigger alarms.",
"B": "Incorrect because manual dashboard checks are not proactive.",
"C": "Incorrect because CloudTrail logs API calls, not arbitrary application log messages.",
"D": "Incorrect because X-Ray focuses on traces and latency, not text pattern detection in logs."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:cloudwatch",
"metric-filters",
"alerts"
]
},
{
"id": "q-d4-ox-009",
"concept_id": "c-ox-dashboards",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "easy",
"question_type": "single",
"stem": "A product manager wants a single view showing API latency, error rates, and DynamoDB throttling metrics. Which AWS feature should the developer use to provide this consolidated view?",
"options": [
{ "label": "A", "text": "CloudWatch Dashboards with multiple widgets." },
{ "label": "B", "text": "CloudTrail event history." },
{ "label": "C", "text": "The S3 console bucket overview." },
{ "label": "D", "text": "An IAM policy document printed as a PDF." }
],
"correct_options": ["A"],
"answer_explanation": "CloudWatch Dashboards allow multiple widgets to be placed on a single dashboard, displaying metrics like API latency, error rates, and DynamoDB throttles in one view. CloudTrail, the S3 console, and IAM policy documents do not provide consolidated metric dashboards.",
"why_this_matters": "Dashboards provide at-a-glance visibility into system health for both technical and non-technical stakeholders. They help teams quickly understand current performance and spot trends.",
"key_takeaway": "Use CloudWatch Dashboards to visualize key metrics from multiple services in one place.",
"option_explanations": {
"A": "Correct because CloudWatch Dashboards aggregate multiple metric widgets into a single view.",
"B": "Incorrect because CloudTrail is for audit logs, not dashboards.",
"C": "Incorrect because the S3 console only shows bucket-specific information.",
"D": "Incorrect because IAM policies are unrelated to performance dashboards."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:cloudwatch",
"dashboards"
]
},
{
"id": "q-d4-ox-010",
"concept_id": "c-ox-root-cause",
"variant_index": 0,
"topic": "observability",
"subtopic": "observability-with-cloudwatch-and-xray",
"domain": "domain-4-troubleshooting-optimization",
"difficulty_inferred": "hard",
"question_type": "single",
"stem": "An application occasionally returns HTTP 500 errors from API Gateway. CloudWatch metrics show an increase in 5XX from the integration. X-Ray traces indicate increased latency in a downstream DynamoDB call just before errors spike. What is the MOST likely next step to identify the root cause?",
"options": [
{ "label": "A", "text": "Examine DynamoDB CloudWatch metrics for throttling, latency, and errors around the same time period." },
{ "label": "B", "text": "Disable X-Ray tracing to reduce overhead and see if errors disappear." },
{ "label": "C", "text": "Delete the API Gateway stage and recreate it from scratch." },
{ "label": "D", "text": "Ignore DynamoDB metrics and focus only on Lambda duration." }
],
"correct_options": ["A"],
"answer_explanation": "X-Ray traces suggest that DynamoDB latency is correlated with API errors. The next step is to review DynamoDB metrics such as read/write capacity, throttling, latency, and error counts to determine if capacity or configuration issues are causing the problem. Disabling X-Ray removes useful diagnostic data. Recreating the API Gateway stage is unlikely to fix a backend latency issue. Focusing only on Lambda duration ignores indicators pointing to DynamoDB.",
"why_this_matters": "Effective root cause analysis requires following evidence across services. Combining tracing data with service-specific metrics helps pinpoint where performance issues originate.",
"key_takeaway": "Use X-Ray traces to guide further investigation into relevant service metrics, such as DynamoDB, when troubleshooting errors.",
"option_explanations": {
"A": "Correct because correlating DynamoDB metrics with X-Ray traces can reveal capacity or throttling issues.",
"B": "Incorrect because disabling tracing removes valuable insights.",
"C": "Incorrect because the issue appears to be downstream, not in the API Gateway configuration.",
"D": "Incorrect because ignoring DynamoDB metrics contradicts the trace evidence."
},
"tags": [
"topic:observability",
"subtopic:observability-with-cloudwatch-and-xray",
"domain:4",
"service:xray",
"service:dynamodb",
"root-cause-analysis"
]
}
]
}
]
}
]
}
]
}
