{
  "exam": "AWS Certified Developer – Associate (DVA-C02)",
  "question_bank_version": "v1.0-continuation",
  "generated_at": "2026-01-11T00:00:00Z",
  "note": "This file continues from Lambda and DynamoDB partition keys, covering remaining topics",
  "domains": [
    {
      "domain_id": "domain-1-development",
      "name": "Development with AWS Services",
      "topics": [
        {
          "topic_id": "dynamodb",
          "name": "Amazon DynamoDB",
          "subtopics": [
            {
              "subtopic_id": "dynamodb-indexes",
              "name": "DynamoDB secondary indexes (GSI and LSI)",
              "num_questions_generated": 10,
              "questions": [
                {
                  "id": "ddb-idx-001",
                  "concept_id": "gsi-vs-lsi",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A developer needs to add a secondary index to an existing DynamoDB table to support a new query pattern. The table already has data and is in production. The new index requires a different partition key than the base table. Which type of index should the developer create?",
                  "options": [
                    {"label": "A", "text": "Local Secondary Index (LSI) because it can be added after table creation"},
                    {"label": "B", "text": "Global Secondary Index (GSI) because it supports a different partition key"},
                    {"label": "C", "text": "Either LSI or GSI will work equally well"},
                    {"label": "D", "text": "Create a new table with the desired partition key instead"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Global Secondary Indexes (GSIs) can be created at any time and support different partition keys from the base table. Local Secondary Indexes (LSIs) must be created at table creation time and must share the same partition key as the base table. Since the requirement is for a different partition key and the table already exists, a GSI is the only option. While creating a new table is possible, it's unnecessary when GSI meets the requirement.",
                  "why_this_matters": "Understanding the differences between GSIs and LSIs is critical for evolving DynamoDB schema to support new access patterns. GSIs provide flexibility for production tables by allowing addition of indexes with different partition keys after creation, enabling applications to adapt to changing requirements without data migration. LSIs are more restrictive but offer strongly consistent reads.",
                  "key_takeaway": "Use Global Secondary Indexes (GSI) when you need a different partition key from the base table or need to add indexes to existing tables; LSIs must be created at table creation and share the base table's partition key.",
                  "option_explanations": {
                    "A": "LSIs must be created at table creation time and cannot have different partition keys from the base table.",
                    "B": "GSIs support different partition keys and can be added to existing tables, meeting both requirements.",
                    "C": "LSIs and GSIs have different constraints; only GSI works for this scenario.",
                    "D": "Creating a new table is unnecessary overhead when GSI provides the needed functionality."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "gsi", "lsi", "indexes"]
                },
                {
                  "id": "ddb-idx-002",
                  "concept_id": "gsi-projection",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A DynamoDB table stores product catalog data with 20 attributes per item. A GSI is created to support searching products by category. Queries on this GSI only need to return 3 attributes: ProductID, Name, and Price. What projection type provides the MOST cost-effective solution?",
                  "options": [
                    {"label": "A", "text": "Use KEYS_ONLY projection"},
                    {"label": "B", "text": "Use INCLUDE projection with ProductID, Name, and Price"},
                    {"label": "C", "text": "Use ALL projection to include all attributes"},
                    {"label": "D", "text": "Use INCLUDE projection with all 20 attributes"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "INCLUDE projection allows you to specify exactly which attributes to project into the GSI. By including only the 3 attributes needed (ProductID, Name, Price), you minimize storage costs for the GSI while ensuring queries can retrieve all required data without fetching from the base table. KEYS_ONLY would require fetching from the base table. ALL projection wastes storage on 17 unnecessary attributes. Including all 20 attributes defeats the purpose of selective projection.",
                  "why_this_matters": "GSI projections directly impact storage costs and query performance. Projecting only needed attributes reduces GSI storage costs while maintaining query efficiency. Over-projecting with ALL wastes money on unused data. Under-projecting with KEYS_ONLY requires expensive base table fetches. Understanding projection optimization is essential for cost-effective DynamoDB design at scale.",
                  "key_takeaway": "Use INCLUDE projection in GSIs to project only the attributes your queries need, minimizing storage costs while avoiding base table fetches for common access patterns.",
                  "option_explanations": {
                    "A": "KEYS_ONLY includes only key attributes, requiring expensive base table fetches for ProductID, Name, and Price.",
                    "B": "INCLUDE projection with specific attributes minimizes storage costs while providing all needed query data.",
                    "C": "ALL projection wastes storage on 17 attributes that queries don't need.",
                    "D": "Including all attributes is identical to ALL projection and wastes storage unnecessarily."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "gsi", "projection", "cost-optimization"]
                },
                {
                  "id": "ddb-idx-003",
                  "concept_id": "lsi-consistency",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A financial application requires strongly consistent reads when querying transaction data by different sort keys. The table uses AccountID as partition key and TransactionID as sort key. The application needs to query transactions by timestamp. What index type supports this requirement?",
                  "options": [
                    {"label": "A", "text": "Global Secondary Index with AccountID as partition key and Timestamp as sort key"},
                    {"label": "B", "text": "Local Secondary Index with AccountID as partition key and Timestamp as sort key"},
                    {"label": "C", "text": "Global Secondary Index with Timestamp as partition key"},
                    {"label": "D", "text": "Either GSI or LSI will provide strongly consistent reads"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Local Secondary Indexes support strongly consistent reads and must share the base table's partition key (AccountID) while providing an alternate sort key (Timestamp). This meets the requirement perfectly. GSIs only support eventually consistent reads. While option A has the right key structure, it's a GSI and doesn't support strong consistency. Option C changes the partition key, which doesn't maintain account-level grouping.",
                  "why_this_matters": "Strong consistency requirements are critical for financial applications where reading outdated data could cause errors. LSIs are the only DynamoDB index type supporting strongly consistent reads, making them essential for use cases requiring read-after-write consistency. Understanding this distinction prevents architectural mistakes in applications with strict consistency requirements.",
                  "key_takeaway": "Use Local Secondary Indexes (LSI) when you need strongly consistent reads with alternate sort keys; GSIs only support eventually consistent reads regardless of configuration.",
                  "option_explanations": {
                    "A": "GSIs only support eventually consistent reads, not strongly consistent reads.",
                    "B": "LSIs support strongly consistent reads and share the base table partition key with alternate sort key.",
                    "C": "Changing partition key to Timestamp doesn't maintain account grouping and GSIs don't support strong consistency.",
                    "D": "Only LSIs support strongly consistent reads; GSIs are always eventually consistent."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "lsi", "consistency", "strong-consistency"]
                },
                {
                  "id": "ddb-idx-004",
                  "concept_id": "sparse-indexes",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "A DynamoDB table stores user accounts where only 5% of users are premium subscribers. The application needs to efficiently query all premium users. The base table has 1 million items. What is the MOST cost-effective indexing strategy?",
                  "options": [
                    {"label": "A", "text": "Create a GSI with SubscriptionType as partition key, projecting all attributes"},
                    {"label": "B", "text": "Create a sparse GSI using PremiumExpiryDate as partition key, only set for premium users"},
                    {"label": "C", "text": "Use a Scan operation with a filter expression for SubscriptionType = 'PREMIUM'"},
                    {"label": "D", "text": "Create a separate table for premium users only"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "A sparse index leverages the fact that GSIs only contain items where the index key attributes are defined. By creating a GSI with PremiumExpiryDate as partition key and only setting this attribute for premium users, the GSI contains only 50,000 items (5% of 1M) instead of all items. This dramatically reduces storage costs and query costs. Option A would index all 1M items. Scan is expensive and slow. A separate table adds operational complexity.",
                  "why_this_matters": "Sparse indexes are a powerful cost optimization technique for scenarios where you need to query a small subset of items. By leveraging DynamoDB's behavior of only indexing items with defined key attributes, you can create indexes containing only relevant items, reducing storage costs and improving query performance. This pattern is especially valuable for large tables with small active subsets.",
                  "key_takeaway": "Create sparse indexes by using GSI key attributes that are only defined for the subset of items you want to index, dramatically reducing index size and costs for querying small subsets.",
                  "option_explanations": {
                    "A": "Indexing SubscriptionType indexes all 1M items with low-cardinality key, wasting storage.",
                    "B": "Sparse index with attribute only set for premium users indexes only 50,000 items, minimizing costs.",
                    "C": "Scan operations are expensive and slow, examining all items regardless of subscription type.",
                    "D": "Separate table adds operational complexity and requires data synchronization logic."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "gsi", "sparse-index", "cost-optimization"]
                },
                {
                  "id": "ddb-idx-005",
                  "concept_id": "gsi-provisioning",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A DynamoDB table uses provisioned capacity mode with 1000 WCU and 1000 RCU. A new GSI is being added. How should the developer configure the GSI's capacity?",
                  "options": [
                    {"label": "A", "text": "GSI automatically inherits the base table's capacity settings"},
                    {"label": "B", "text": "GSI requires separate capacity configuration independent of the base table"},
                    {"label": "C", "text": "GSI shares the base table's capacity pool"},
                    {"label": "D", "text": "GSI capacity cannot be configured separately"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "In provisioned capacity mode, each GSI requires its own separate read and write capacity allocation, independent of the base table. When planning GSI capacity, consider that writes to the base table also consume GSI write capacity (for each updated item that affects the GSI). GSI capacity doesn't automatically inherit or share base table capacity. Each GSI must be provisioned independently based on expected query and write patterns.",
                  "why_this_matters": "Understanding that GSIs require separate capacity provisioning is critical for capacity planning and cost management. A heavily queried GSI might need more read capacity than the base table, while GSIs receiving updates from every base table write need adequate write capacity. Failing to provision GSI capacity independently leads to throttling, even when the base table has adequate capacity.",
                  "key_takeaway": "Global Secondary Indexes require separate capacity provisioning in provisioned mode—plan GSI capacity based on query patterns and base table write volume affecting the GSI.",
                  "option_explanations": {
                    "A": "GSIs do not inherit capacity; they require independent capacity configuration.",
                    "B": "Each GSI needs separate read and write capacity units configured independently from the base table.",
                    "C": "GSIs have separate capacity pools; they don't share the base table's capacity.",
                    "D": "GSI capacity must be configured separately for each GSI in provisioned mode."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "gsi", "provisioned-capacity", "capacity-planning"]
                },
                {
                  "id": "ddb-idx-006",
                  "concept_id": "gsi-backfilling",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A developer adds a new GSI to a DynamoDB table containing 10 million items. What happens during the GSI creation process?",
                  "options": [
                    {"label": "A", "text": "The GSI becomes immediately available and queryable"},
                    {"label": "B", "text": "DynamoDB backfills the GSI by scanning the base table and populating the index; queries wait until completion"},
                    {"label": "C", "text": "The base table becomes read-only until GSI creation completes"},
                    {"label": "D", "text": "GSI creation fails because indexes cannot be added to tables with existing data"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "When creating a GSI on a table with existing data, DynamoDB backfills the index by scanning the base table and populating the GSI. The GSI status is CREATING during this process, and queries against it will fail. The base table remains fully available for reads and writes. For large tables, backfilling can take significant time. Once complete, the GSI becomes ACTIVE and queryable. The base table is never made read-only, and GSIs can be added to tables with any amount of existing data.",
                  "why_this_matters": "Understanding GSI backfilling behavior is essential for planning index additions to production tables. Large tables may take hours to backfill, during which the GSI is unusable. Applications must handle this gracefully, potentially using feature flags or phased rollouts. Knowing that the base table remains available prevents unnecessary downtime concerns when adding GSIs to production systems.",
                  "key_takeaway": "Adding GSIs to existing tables triggers a backfill process that scans the base table; the GSI is unavailable until backfilling completes, but the base table remains fully operational.",
                  "option_explanations": {
                    "A": "GSIs require backfilling from existing base table data before becoming queryable.",
                    "B": "DynamoDB backfills new GSIs by scanning the base table; the GSI is unavailable during CREATING status until backfill completes.",
                    "C": "The base table remains fully available for all operations during GSI creation and backfilling.",
                    "D": "GSIs can be added to tables with any amount of existing data; DynamoDB handles backfilling automatically."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "gsi", "backfilling", "index-creation"]
                },
                {
                  "id": "ddb-idx-007",
                  "concept_id": "index-overloading",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "A DynamoDB table stores multiple entity types (Users, Orders, Products) using a single table design. The table uses a generic partition key 'PK' and sort key 'SK'. Which indexing strategy supports querying each entity type by different attributes efficiently?",
                  "options": [
                    {"label": "A", "text": "Create a separate GSI for each entity type"},
                    {"label": "B", "text": "Create a single overloaded GSI with generic key names that hold different values for different entity types"},
                    {"label": "C", "text": "Use the base table keys and filter expressions"},
                    {"label": "D", "text": "Create separate tables for each entity type"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Index overloading is a single-table design pattern where one GSI serves multiple entity types by using generic attribute names (like GSI1PK, GSI1SK) that contain different semantic values for different entity types. For Users, GSI1PK might be 'EMAIL#user@example.com'; for Orders, 'STATUS#PENDING'. This maximizes the 20-GSI limit. Creating separate GSIs per entity wastes index quota. Filter expressions require scanning. Separate tables defeat single-table design benefits.",
                  "why_this_matters": "Single-table design is a DynamoDB best practice for related entities, reducing costs and operational complexity. Index overloading enables this pattern to scale to many entity types and access patterns within the 20-GSI limit. Understanding this advanced pattern is essential for building sophisticated applications that leverage DynamoDB's strengths while working within its constraints.",
                  "key_takeaway": "Use index overloading with generic GSI key attributes (GSI1PK, GSI1SK) that store different values for different entity types to support multiple access patterns within the 20-GSI limit in single-table designs.",
                  "option_explanations": {
                    "A": "Creating separate GSIs per entity quickly exhausts the 20-GSI limit and doesn't scale.",
                    "B": "Overloaded GSIs with generic keys serve multiple entity types efficiently, maximizing the GSI limit.",
                    "C": "Filter expressions require scanning, which is expensive and slow for large tables.",
                    "D": "Separate tables increase costs and operational complexity, defeating single-table design benefits."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "gsi", "single-table-design", "index-overloading"]
                },
                {
                  "id": "ddb-idx-008",
                  "concept_id": "lsi-limits",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A developer is designing a new DynamoDB table and wants to add multiple Local Secondary Indexes. What is the maximum number of LSIs that can be created on a DynamoDB table?",
                  "options": [
                    {"label": "A", "text": "5 LSIs per table"},
                    {"label": "B", "text": "10 LSIs per table"},
                    {"label": "C", "text": "20 LSIs per table"},
                    {"label": "D", "text": "Unlimited LSIs"}
                  ],
                  "correct_options": ["A"],
                  "answer_explanation": "DynamoDB supports a maximum of 5 Local Secondary Indexes per table. This is a hard limit that cannot be increased. LSIs must be created at table creation time and cannot be added later. In contrast, you can have up to 20 Global Secondary Indexes per table. The LSI limit is lower because LSIs share partition space with the base table and can impact partition size limits.",
                  "why_this_matters": "The 5-LSI limit is a critical constraint in table design that requires careful planning of strongly consistent secondary access patterns. Since LSIs cannot be added after table creation, you must identify all strongly consistent query patterns upfront. Understanding this limit prevents table redesigns and guides decisions between LSIs and GSIs during initial schema design.",
                  "key_takeaway": "DynamoDB tables are limited to 5 Local Secondary Indexes that must be created at table creation time—plan strongly consistent query patterns carefully as LSIs cannot be added later.",
                  "option_explanations": {
                    "A": "DynamoDB supports a maximum of 5 LSIs per table, a hard limit.",
                    "B": "10 is not the LSI limit; the actual limit is 5 LSIs per table.",
                    "C": "20 is the GSI limit, not the LSI limit which is 5.",
                    "D": "LSIs have a hard limit of 5 per table, not unlimited."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "lsi", "limits"]
                },
                {
                  "id": "ddb-idx-009",
                  "concept_id": "gsi-write-costs",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A DynamoDB table has 3 Global Secondary Indexes. When an item is written to the base table and the write affects all 3 GSIs, how many write operations are consumed?",
                  "options": [
                    {"label": "A", "text": "1 write operation (base table only)"},
                    {"label": "B", "text": "2 write operations (base table + indexes combined)"},
                    {"label": "C", "text": "4 write operations (1 base table + 3 GSIs)"},
                    {"label": "D", "text": "3 write operations (indexes only)"}
                  ],
                  "correct_options": ["C"],
                  "answer_explanation": "When writing to a DynamoDB table with GSIs, you consume one write for the base table plus one write for each GSI that is affected by the change. If an item write affects all 3 GSIs (because the item has the GSI partition keys defined), you consume 4 total writes: 1 for the base table + 3 for the GSIs. This multiplicative effect significantly impacts write costs and capacity planning for tables with many GSIs.",
                  "why_this_matters": "Understanding GSI write costs is critical for capacity planning and cost optimization. Each GSI that indexes an item multiplies write costs. Tables with many GSIs can consume 5-10x more write capacity than the base table alone. This knowledge guides decisions about how many GSIs to create, projection strategies, and whether to use sparse indexes to limit which items are indexed.",
                  "key_takeaway": "Each write to a DynamoDB item consumes write capacity for the base table plus one write per GSI affected by the change—plan capacity accounting for this multiplication factor.",
                  "option_explanations": {
                    "A": "GSI writes are not free; each affected GSI consumes additional write capacity.",
                    "B": "Each GSI affected by the write consumes separate write capacity, not combined.",
                    "C": "One write for base table plus one write per affected GSI equals 4 total writes.",
                    "D": "The base table write is always counted in addition to GSI writes."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "gsi", "write-costs", "capacity-planning"]
                },
                {
                  "id": "ddb-idx-010",
                  "concept_id": "index-key-schema",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-indexes",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "multi",
                  "stem": "A developer is creating a GSI for a DynamoDB table. Which TWO statements about GSI key schema are correct? (Select TWO)",
                  "options": [
                    {"label": "A", "text": "The GSI partition key must be different from the base table partition key"},
                    {"label": "B", "text": "The GSI can use any base table attribute as its partition key or sort key"},
                    {"label": "C", "text": "The GSI must include the base table's primary key attributes in its projection"},
                    {"label": "D", "text": "The GSI sort key is optional"}
                  ],
                  "correct_options": ["B", "D"],
                  "answer_explanation": "GSIs can use any scalar attribute from the base table as their partition or sort key, providing flexibility for different access patterns. The GSI partition key can be the same as or different from the base table's partition key. GSI sort keys are optional—you can create a GSI with only a partition key. DynamoDB automatically includes the base table's primary key in GSI projections regardless of projection type, so you don't need to explicitly include them.",
                  "why_this_matters": "Understanding GSI key schema flexibility enables effective index design for diverse access patterns. The ability to use any attribute as GSI keys and make sort keys optional provides powerful querying capabilities. Knowing that base table keys are automatically projected prevents redundant attribute specification and ensures you can always retrieve full items from GSI queries.",
                  "key_takeaway": "GSIs can use any base table attribute as partition or sort key, sort keys are optional, and base table primary keys are automatically projected to all GSIs regardless of projection settings.",
                  "option_explanations": {
                    "A": "GSI partition keys can be the same as or different from the base table partition key.",
                    "B": "Any scalar base table attribute can serve as a GSI partition or sort key, providing query flexibility.",
                    "C": "Base table primary keys are automatically included in GSI projections; explicit inclusion is unnecessary.",
                    "D": "GSI sort keys are optional; partition-key-only GSIs are valid and useful for many access patterns."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-indexes", "domain:1", "service:dynamodb", "gsi", "key-schema", "design"]
                }
              ]
            },
            {
              "subtopic_id": "dynamodb-consistency",
              "name": "DynamoDB consistency models and read operations",
              "num_questions_generated": 10,
              "questions": [
                {
                  "id": "ddb-cons-001",
                  "concept_id": "eventual-vs-strong-consistency",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A banking application writes a transaction to DynamoDB and immediately reads it back to confirm the write. The application occasionally reads stale data. What is the MOST likely cause?",
                  "options": [
                    {"label": "A", "text": "The application is using eventually consistent reads instead of strongly consistent reads"},
                    {"label": "B", "text": "DynamoDB is experiencing replication lag due to high load"},
                    {"label": "C", "text": "The table is configured with eventual consistency mode"},
                    {"label": "D", "text": "The application needs to wait at least 1 second between write and read"}
                  ],
                  "correct_options": ["A"],
                  "answer_explanation": "Eventually consistent reads may return stale data because they don't guarantee reading the most recent write. Strongly consistent reads ensure you get the latest data. DynamoDB doesn't have table-level consistency modes—consistency is specified per read operation. While DynamoDB replicates data across availability zones, this is typically sub-second and isn't configurable. No minimum wait time is required; strongly consistent reads immediately reflect writes.",
                  "why_this_matters": "Understanding consistency models is critical for applications requiring read-after-write consistency, such as financial systems, inventory management, and booking systems. Eventually consistent reads are cheaper (consume half the RCU) but may return stale data. Strongly consistent reads guarantee current data but cost more. Choosing the wrong consistency model can lead to data integrity issues or unnecessary costs.",
                  "key_takeaway": "Use strongly consistent reads when you need to immediately read your own writes or require the latest data; use eventually consistent reads for cost savings when stale data is acceptable.",
                  "option_explanations": {
                    "A": "Eventually consistent reads can return stale data; strongly consistent reads guarantee the most recent write is reflected.",
                    "B": "DynamoDB's replication is sub-second and not configurable; consistency choice determines read behavior.",
                    "C": "Consistency is set per read operation, not at the table level.",
                    "D": "Strongly consistent reads immediately reflect writes without requiring wait times."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "consistency", "reads"]
                },
                {
                  "id": "ddb-cons-002",
                  "concept_id": "gsi-consistency-limitation",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A developer needs to query a DynamoDB Global Secondary Index and requires strongly consistent reads. What will happen?",
                  "options": [
                    {"label": "A", "text": "The query will succeed with strongly consistent reads"},
                    {"label": "B", "text": "The query will fail because GSIs only support eventually consistent reads"},
                    {"label": "C", "text": "The query will automatically fall back to the base table for strongly consistent reads"},
                    {"label": "D", "text": "The query will succeed but with higher latency"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Global Secondary Indexes only support eventually consistent reads. If you specify ConsistentRead=true when querying a GSI, DynamoDB will return an error. This is a fundamental limitation of GSIs. If strong consistency is required, you must either query the base table directly using its primary key, or use a Local Secondary Index (which does support strongly consistent reads). GSIs cannot automatically fall back to base table reads.",
                  "why_this_matters": "Understanding that GSIs don't support strong consistency is critical for applications with consistency requirements. This limitation influences index type selection—if you need strongly consistent reads with alternate sort keys, you must use LSIs instead. For many applications, eventual consistency on GSIs is acceptable, but financial, inventory, or booking systems may require LSI or base table queries.",
                  "key_takeaway": "Global Secondary Indexes only support eventually consistent reads—use Local Secondary Indexes or base table queries if you require strongly consistent reads.",
                  "option_explanations": {
                    "A": "GSIs do not support strongly consistent reads; the operation will fail.",
                    "B": "GSIs only support eventually consistent reads; requesting strong consistency returns an error.",
                    "C": "DynamoDB doesn't automatically fall back to base table; the query fails if strong consistency is requested on a GSI.",
                    "D": "The query doesn't succeed with higher latency; it fails because GSIs don't support strong consistency."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "gsi", "consistency", "limitations"]
                },
                {
                  "id": "ddb-cons-003",
                  "concept_id": "consistency-cost",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "An application reads 1000 items per second from DynamoDB, each item being 4 KB. If the application switches from eventually consistent reads to strongly consistent reads, how will read capacity consumption change?",
                  "options": [
                    {"label": "A", "text": "No change in capacity consumption"},
                    {"label": "B", "text": "Read capacity consumption will double"},
                    {"label": "C", "text": "Read capacity consumption will be reduced by half"},
                    {"label": "D", "text": "Read capacity consumption will increase by 50%"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Strongly consistent reads consume twice the read capacity units of eventually consistent reads. For a 4 KB item, an eventually consistent read consumes 1 RCU while a strongly consistent read consumes 2 RCU. Therefore, switching from eventual to strong consistency doubles read costs. This is an important cost consideration—if eventual consistency is acceptable, you can serve twice the traffic with the same capacity budget.",
                  "why_this_matters": "The 2x cost difference between consistency modes significantly impacts both provisioned capacity planning and on-demand pricing. For read-heavy applications where eventual consistency is acceptable (e.g., product catalogs, social feeds), using eventually consistent reads cuts read costs in half. Understanding this cost tradeoff helps optimize spending while meeting application requirements.",
                  "key_takeaway": "Strongly consistent reads consume twice the read capacity of eventually consistent reads—use eventual consistency when acceptable to reduce read costs by 50%.",
                  "option_explanations": {
                    "A": "Consistency mode directly impacts RCU consumption; there is a cost difference.",
                    "B": "Strongly consistent reads consume 2x the RCU of eventually consistent reads for the same data.",
                    "C": "Strong consistency increases costs; it doesn't reduce them.",
                    "D": "The increase is 100% (double), not 50%."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "consistency", "cost", "rcu"]
                },
                {
                  "id": "ddb-cons-004",
                  "concept_id": "transactional-read-consistency",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A developer uses DynamoDB TransactGetItems to read multiple items in a single atomic operation. What consistency model do transactional reads provide?",
                  "options": [
                    {"label": "A", "text": "Eventually consistent reads"},
                    {"label": "B", "text": "Strongly consistent reads"},
                    {"label": "C", "text": "Configurable consistency per item in the transaction"},
                    {"label": "D", "text": "Serializable isolation"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "DynamoDB transactional reads (TransactGetItems) always use strongly consistent reads. You cannot configure them to use eventual consistency. This ensures that all items in the transaction reflect their most recent committed state. While DynamoDB transactions provide serializable isolation for the transaction as a whole, the question specifically asks about the consistency model, which is strongly consistent.",
                  "why_this_matters": "Understanding that transactional reads are always strongly consistent is important for capacity planning and cost estimation. Transactional reads consume 2 RCUs per item (same as regular strongly consistent reads) plus potential additional costs for the transactional guarantee. You cannot save costs by using eventual consistency in transactions—if you don't need atomicity, use BatchGetItem with eventual consistency instead.",
                  "key_takeaway": "DynamoDB transactional reads always use strongly consistent reads and cannot be configured for eventual consistency—factor this into capacity planning and cost estimates.",
                  "option_explanations": {
                    "A": "Transactional reads always use strong consistency, not eventual consistency.",
                    "B": "TransactGetItems always performs strongly consistent reads across all items in the transaction.",
                    "C": "Consistency cannot be configured per item in transactional reads; all reads are strongly consistent.",
                    "D": "While transactions provide serializable isolation, the consistency model is specifically strongly consistent reads."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "transactions", "consistency"]
                },
                {
                  "id": "ddb-cons-005",
                  "concept_id": "cross-region-consistency",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "An application uses DynamoDB Global Tables with replicas in us-east-1 and eu-west-1. A write is made to the us-east-1 replica. What consistency guarantee does a strongly consistent read from the eu-west-1 replica provide?",
                  "options": [
                    {"label": "A", "text": "The read will immediately reflect the write from us-east-1"},
                    {"label": "B", "text": "The read will reflect the latest write to the eu-west-1 replica, but not necessarily the us-east-1 write"},
                    {"label": "C", "text": "Global Tables do not support strongly consistent reads"},
                    {"label": "D", "text": "The read will wait until the us-east-1 write replicates to eu-west-1"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Strongly consistent reads guarantee consistency within a single region only—they reflect the most recent write to that specific replica. Cross-region replication in Global Tables is asynchronous, typically completing in under a second. A strongly consistent read from eu-west-1 returns the latest data written to eu-west-1, not necessarily reflecting concurrent writes to us-east-1. Global Tables do support strongly consistent reads, but the consistency guarantee is region-scoped, not global.",
                  "why_this_matters": "Understanding the regional scope of strong consistency is critical for globally distributed applications. Global Tables provide high availability and low latency through multi-region replication, but don't provide global strong consistency. Applications requiring global consistency across regions need application-level coordination or different architecture patterns. This knowledge prevents incorrect assumptions about data consistency in multi-region deployments.",
                  "key_takeaway": "Strong consistency in DynamoDB Global Tables is region-scoped—reads are strongly consistent within a region but cannot guarantee immediate visibility of writes from other regions due to asynchronous replication.",
                  "option_explanations": {
                    "A": "Cross-region replication is asynchronous; strongly consistent reads don't wait for or guarantee visibility of other regions' writes.",
                    "B": "Strongly consistent reads are region-scoped, reflecting the latest write to that replica, not necessarily cross-region writes.",
                    "C": "Global Tables support strongly consistent reads, but the consistency is region-scoped.",
                    "D": "Strongly consistent reads don't wait for cross-region replication; they return the latest regional data immediately."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "global-tables", "consistency", "multi-region"]
                },
                {
                  "id": "ddb-cons-006",
                  "concept_id": "read-consistency-sdk",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A developer is using the AWS SDK to query a DynamoDB table. What is the default read consistency mode if ConsistentRead is not specified in the query?",
                  "options": [
                    {"label": "A", "text": "Strongly consistent reads"},
                    {"label": "B", "text": "Eventually consistent reads"},
                    {"label": "C", "text": "The table's configured default consistency mode"},
                    {"label": "D", "text": "Transactional reads"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "DynamoDB uses eventually consistent reads by default when the ConsistentRead parameter is not specified or is set to false. To use strongly consistent reads, you must explicitly set ConsistentRead=true in your query or get item request. There is no table-level consistency configuration—consistency is chosen per read operation. Transactional reads require using the TransactGetItems API, not regular Query or GetItem operations.",
                  "why_this_matters": "Knowing the default consistency behavior prevents unintended stale reads in applications requiring strong consistency. Many developers assume SDK reads are strongly consistent by default, leading to subtle bugs where applications occasionally see stale data. Always explicitly setting ConsistentRead based on application requirements ensures predictable behavior and prevents consistency-related issues.",
                  "key_takeaway": "DynamoDB reads are eventually consistent by default—explicitly set ConsistentRead=true when you need strongly consistent reads to avoid unintended stale data.",
                  "option_explanations": {
                    "A": "Strongly consistent reads require explicit ConsistentRead=true parameter; they're not the default.",
                    "B": "Eventually consistent reads are the default when ConsistentRead is not specified or is false.",
                    "C": "Consistency is set per read operation, not configured at the table level.",
                    "D": "Transactional reads require using TransactGetItems API, not default query behavior."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "consistency", "sdk", "defaults"]
                },
                {
                  "id": "ddb-cons-007",
                  "concept_id": "batch-read-consistency",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A developer uses BatchGetItem to retrieve 25 items from a DynamoDB table. Half the items should use eventually consistent reads and half should use strongly consistent reads. How should this be implemented?",
                  "options": [
                    {"label": "A", "text": "Set ConsistentRead parameter differently for each item in the batch"},
                    {"label": "B", "text": "Make two separate BatchGetItem calls, one with ConsistentRead=false and one with ConsistentRead=true"},
                    {"label": "C", "text": "Use TransactGetItems with mixed consistency settings"},
                    {"label": "D", "text": "BatchGetItem doesn't support strongly consistent reads"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "BatchGetItem applies a single consistency setting to all items in the batch request. You cannot mix consistency models within a single BatchGetItem call. To read some items with eventual consistency and others with strong consistency, you must make two separate BatchGetItem calls with different ConsistentRead settings. TransactGetItems always uses strong consistency and cannot be configured for mixed consistency. BatchGetItem does support strongly consistent reads when ConsistentRead=true is specified.",
                  "why_this_matters": "Understanding BatchGetItem consistency limitations is important for optimizing batch read operations. If you need different consistency models for different items, you must split them into separate batch calls. This impacts both application code structure and performance characteristics. Many developers incorrectly assume per-item consistency configuration is possible, leading to incorrect implementations.",
                  "key_takeaway": "BatchGetItem applies one consistency setting to all items in the batch—use separate batch calls when you need different consistency models for different items.",
                  "option_explanations": {
                    "A": "BatchGetItem doesn't support per-item consistency configuration; one setting applies to the entire batch.",
                    "B": "Separate BatchGetItem calls with different ConsistentRead settings are required for mixed consistency needs.",
                    "C": "TransactGetItems always uses strong consistency and doesn't support mixed or eventual consistency.",
                    "D": "BatchGetItem supports strongly consistent reads when ConsistentRead=true is specified for the batch."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "consistency", "batch-operations"]
                },
                {
                  "id": "ddb-cons-008",
                  "concept_id": "scan-consistency",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A developer runs a Scan operation on a DynamoDB table with ConsistentRead=true. What behavior should they expect?",
                  "options": [
                    {"label": "A", "text": "The scan will return all items as of the moment the scan started"},
                    {"label": "B", "text": "The scan will return strongly consistent data for each item as it's read"},
                    {"label": "C", "text": "Scan operations do not support strongly consistent reads"},
                    {"label": "D", "text": "The scan will use transactional isolation"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Scan operations support strongly consistent reads via the ConsistentRead parameter. When set to true, each item is read with strong consistency as the scan progresses. However, scans are not atomic snapshots—items modified during the scan may or may not be included depending on when they're accessed. Strong consistency means each individual item reflects recent writes, but doesn't create a point-in-time snapshot of the entire table. Scans don't use transactional isolation.",
                  "why_this_matters": "Understanding scan consistency behavior is important for applications that scan tables for reporting or analytics. While strongly consistent scans ensure each item reflects recent writes, they don't provide snapshot isolation. Items modified during long-running scans may be seen in their old or new state depending on timing. This knowledge helps developers set correct expectations for scan results and choose appropriate tools for consistent snapshots.",
                  "key_takeaway": "Scan operations support strongly consistent reads on a per-item basis but don't provide atomic snapshots—items can be modified during the scan with unpredictable results.",
                  "option_explanations": {
                    "A": "Scans don't create point-in-time snapshots; items can change during the scan.",
                    "B": "ConsistentRead=true makes each item strongly consistent as it's read, though not as an atomic snapshot.",
                    "C": "Scan operations support strongly consistent reads via the ConsistentRead parameter.",
                    "D": "Scans don't provide transactional isolation or snapshot consistency, even with ConsistentRead=true."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "scan", "consistency"]
                },
                {
                  "id": "ddb-cons-009",
                  "concept_id": "conditional-write-consistency",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A developer uses a conditional write (PutItem with ConditionExpression) to update an item only if a specific attribute value matches the expected value. What consistency guarantee does the condition check provide?",
                  "options": [
                    {"label": "A", "text": "The condition is checked against eventually consistent data"},
                    {"label": "B", "text": "The condition is checked against strongly consistent data"},
                    {"label": "C", "text": "The consistency depends on the ConsistentRead parameter"},
                    {"label": "D", "text": "Conditional writes don't guarantee any specific consistency"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Conditional writes always evaluate conditions against strongly consistent data. This ensures the condition check sees the most recent committed value, preventing race conditions. There is no ConsistentRead parameter for writes—DynamoDB always uses strong consistency for condition evaluation to maintain data integrity. This behavior is automatic and cannot be configured otherwise.",
                  "why_this_matters": "Understanding that conditional writes use strong consistency is essential for implementing optimistic locking and preventing race conditions. The strong consistency guarantee ensures that condition checks accurately reflect the current item state, enabling safe concurrent updates. This is fundamental to building correct distributed systems with DynamoDB where multiple clients might update the same items.",
                  "key_takeaway": "Conditional writes always evaluate conditions against strongly consistent data, ensuring accurate condition checks and preventing race conditions in concurrent update scenarios.",
                  "option_explanations": {
                    "A": "Conditional writes always use strong consistency for condition evaluation, not eventual consistency.",
                    "B": "Conditions are evaluated against strongly consistent data to ensure accurate checks and prevent race conditions.",
                    "C": "There is no ConsistentRead parameter for writes; strong consistency is always used for condition evaluation.",
                    "D": "Conditional writes guarantee strong consistency for condition checks to maintain data integrity."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "conditional-writes", "consistency"]
                },
                {
                  "id": "ddb-cons-010",
                  "concept_id": "consistency-best-practices",
                  "variant_index": 0,
                  "topic": "dynamodb",
                  "subtopic": "dynamodb-consistency",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "hard",
                  "question_type": "multi",
                  "stem": "A high-traffic e-commerce application uses DynamoDB to store product inventory. Which TWO scenarios should use strongly consistent reads? (Select TWO)",
                  "options": [
                    {"label": "A", "text": "Displaying product details on product listing pages"},
                    {"label": "B", "text": "Checking inventory availability during checkout before payment processing"},
                    {"label": "C", "text": "Reading user's order history for display on account page"},
                    {"label": "D", "text": "Verifying inventory levels immediately after updating stock quantities"}
                  ],
                  "correct_options": ["B", "D"],
                  "answer_explanation": "Checking inventory during checkout requires strongly consistent reads to ensure accurate availability before processing payment—stale data could lead to overselling. Verifying inventory after updates also requires strong consistency to confirm the write succeeded. Product listing pages can use eventual consistency since slightly stale product details don't cause critical issues. Order history display tolerates eventual consistency as historical data doesn't require immediate accuracy.",
                  "why_this_matters": "Choosing appropriate consistency levels balances cost and correctness. Over-using strong consistency wastes money on double RCU costs for reads where stale data is acceptable. Under-using it causes data integrity issues in critical paths like payment processing or inventory management. Understanding which operations truly require strong consistency is essential for building cost-effective, correct applications.",
                  "key_takeaway": "Use strongly consistent reads for critical operations requiring accuracy (checkout, post-write verification); use eventually consistent reads for display and non-critical operations to reduce costs.",
                  "option_explanations": {
                    "A": "Product listing pages tolerate slightly stale data; eventual consistency reduces costs without impacting user experience.",
                    "B": "Checkout requires accurate inventory to prevent overselling; strongly consistent reads ensure correct availability data.",
                    "C": "Historical order data doesn't require immediate consistency; eventual consistency is acceptable for display.",
                    "D": "Post-write verification requires strong consistency to confirm the update succeeded and see current state."
                  },
                  "tags": ["topic:dynamodb", "subtopic:dynamodb-consistency", "domain:1", "service:dynamodb", "consistency", "best-practices", "use-cases"]
                }
              ]
            }
          ]
        },
        {
          "topic_id": "api-gateway",
          "name": "Amazon API Gateway",
          "subtopics": [
            {
              "subtopic_id": "api-gateway-integration",
              "name": "API Gateway integration types and request/response transformation",
              "num_questions_generated": 10,
              "questions": [
                {
                  "id": "apigw-int-001",
                  "concept_id": "integration-types",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A developer is creating an API Gateway REST API that proxies requests directly to a Lambda function without any request/response transformation. Which integration type should the developer use?",
                  "options": [
                    {"label": "A", "text": "AWS integration"},
                    {"label": "B", "text": "AWS_PROXY integration"},
                    {"label": "C", "text": "HTTP integration"},
                    {"label": "D", "text": "MOCK integration"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "AWS_PROXY (also called Lambda proxy integration) passes the entire request to Lambda as a structured event and expects a specifically formatted response. This eliminates the need for integration request/response mapping templates. AWS integration requires explicit mapping templates for request/response transformation. HTTP integration is for HTTP endpoints, not Lambda. MOCK integration returns responses without calling a backend.",
                  "why_this_matters": "Understanding integration types is fundamental to API Gateway development. Lambda proxy integration is the simplest and most common pattern, reducing configuration complexity by delegating request/response handling to Lambda code. This simplifies development and reduces API Gateway configuration errors, making it the recommended approach for most Lambda-backed APIs.",
                  "key_takeaway": "Use AWS_PROXY (Lambda proxy) integration for Lambda functions to simplify configuration by handling request/response transformation in Lambda code rather than API Gateway mapping templates.",
                  "option_explanations": {
                    "A": "AWS integration requires explicit mapping templates for request/response transformation, not direct proxying.",
                    "B": "AWS_PROXY integration directly passes requests to Lambda and expects Lambda to format responses, eliminating mapping templates.",
                    "C": "HTTP integration is for HTTP endpoints, not Lambda functions.",
                    "D": "MOCK integration returns static responses without invoking any backend service."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "service:api-gateway", "service:lambda", "integration-types"]
                },
                {
                  "id": "apigw-int-002",
                  "concept_id": "mapping-templates",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "An API Gateway REST API receives JSON requests but needs to transform them into XML format before passing to a SOAP-based backend service. What feature should the developer use?",
                  "options": [
                    {"label": "A", "text": "Request validators"},
                    {"label": "B", "text": "Integration request mapping templates using VTL (Velocity Template Language)"},
                    {"label": "C", "text": "Lambda authorizers to transform the request"},
                    {"label": "D", "text": "Method response headers"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Integration request mapping templates using Velocity Template Language (VTL) transform incoming requests before they reach the backend integration. This is the proper way to convert JSON to XML or perform other request transformations. Request validators check request format but don't transform it. Lambda authorizers handle authentication/authorization, not request transformation. Method response headers configure response metadata, not request transformation.",
                  "why_this_matters": "Request transformation is common when integrating modern REST APIs with legacy SOAP or other protocols. Mapping templates in API Gateway enable protocol translation without requiring additional Lambda functions or proxy servers, reducing latency and costs. Understanding VTL mapping templates is essential for building APIs that bridge different systems and protocols.",
                  "key_takeaway": "Use integration request mapping templates with Velocity Template Language to transform requests (e.g., JSON to XML) before they reach backend integrations.",
                  "option_explanations": {
                    "A": "Request validators validate request format but don't transform request content or structure.",
                    "B": "Integration request mapping templates with VTL transform request format and structure before backend invocation.",
                    "C": "Lambda authorizers perform authentication/authorization, not request content transformation.",
                    "D": "Method response headers configure response metadata, not request transformation."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "service:api-gateway", "mapping-templates", "vtl", "transformation"]
                },
                {
                  "id": "apigw-int-003",
                  "concept_id": "http-integration",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "An API Gateway REST API needs to call a public HTTP API endpoint and transform both the request and response. Which integration type provides the MOST control over request/response transformation?",
                  "options": [
                    {"label": "A", "text": "HTTP integration"},
                    {"label": "B", "text": "HTTP_PROXY integration"},
                    {"label": "C", "text": "AWS integration"},
                    {"label": "D", "text": "VPC_LINK integration"}
                  ],
                  "correct_options": ["A"],
                  "answer_explanation": "HTTP integration allows full control over request and response transformation using mapping templates. HTTP_PROXY passes requests through without transformation capability. AWS integration is for AWS services, not HTTP endpoints. VPC_LINK is for accessing HTTP APIs in VPCs, but when using proxy mode, it doesn't allow transformations. Standard HTTP integration with mapping templates provides maximum transformation control.",
                  "why_this_matters": "Choosing between HTTP and HTTP_PROXY integration types affects your ability to transform requests and responses. When integrating with external APIs that require different data formats, authentication headers, or response structure changes, HTTP integration with mapping templates is essential. HTTP_PROXY is simpler but inflexible for transformation needs.",
                  "key_takeaway": "Use HTTP integration (not HTTP_PROXY) when you need to transform requests or responses for external HTTP endpoints; proxy mode eliminates transformation capabilities.",
                  "option_explanations": {
                    "A": "HTTP integration enables request/response transformation via mapping templates while calling HTTP endpoints.",
                    "B": "HTTP_PROXY passes requests through to HTTP backends without allowing transformation via mapping templates.",
                    "C": "AWS integration is for calling AWS services, not public HTTP endpoints.",
                    "D": "VPC_LINK is for private HTTP endpoints in VPCs; proxy mode doesn't support transformations."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "service:api-gateway", "http-integration", "transformation"]
                },
                {
                  "id": "apigw-int-004",
                  "concept_id": "mock-integration",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A development team wants to test API Gateway configuration before implementing backend Lambda functions. What integration type allows returning static responses for testing?",
                  "options": [
                    {"label": "A", "text": "AWS integration"},
                    {"label": "B", "text": "HTTP integration"},
                    {"label": "C", "text": "MOCK integration"},
                    {"label": "D", "text": "Lambda proxy integration"}
                  ],
                  "correct_options": ["C"],
                  "answer_explanation": "MOCK integration returns responses directly from API Gateway without invoking any backend service. This is ideal for testing API structure, request validation, and response mapping before implementing backend logic. You configure the mock response using integration response mapping templates. AWS, HTTP, and Lambda integrations all require actual backend services to be available.",
                  "why_this_matters": "MOCK integration enables API-first development where API contracts are defined and tested before backend implementation begins. This allows frontend and backend teams to work in parallel using agreed-upon API specifications. Mock endpoints also serve as examples in API documentation and enable testing of API Gateway features like request validation and response transformation without backend dependencies.",
                  "key_takeaway": "Use MOCK integration to return static responses for testing API Gateway configuration and enabling API-first development without requiring backend implementations.",
                  "option_explanations": {
                    "A": "AWS integration requires an actual AWS service backend to invoke.",
                    "B": "HTTP integration requires an actual HTTP endpoint to call.",
                    "C": "MOCK integration returns configured static responses without invoking any backend, ideal for testing.",
                    "D": "Lambda proxy integration requires an actual Lambda function to be implemented and deployed."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "domain:3", "service:api-gateway", "mock-integration", "testing"]
                },
                {
                  "id": "apigw-int-005",
                  "concept_id": "integration-timeout",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "An API Gateway REST API integrates with a Lambda function that occasionally takes 45 seconds to process requests. Clients are receiving 504 Gateway Timeout errors. What is the MOST likely cause?",
                  "options": [
                    {"label": "A", "text": "Lambda function timeout is set to 3 seconds"},
                    {"label": "B", "text": "API Gateway has a maximum integration timeout of 29 seconds"},
                    {"label": "C", "text": "The Lambda function is being throttled"},
                    {"label": "D", "text": "API Gateway request body size limit is exceeded"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "API Gateway REST APIs have a hard limit of 29 seconds for integration timeout. Any backend integration taking longer than 29 seconds will result in a 504 Gateway Timeout error. This limit cannot be increased. For operations requiring more than 29 seconds, consider using asynchronous patterns with Step Functions, SQS, or returning a job ID for status polling. Lambda timeout would cause different errors. Throttling causes 429 errors. Body size limits cause 413 errors.",
                  "why_this_matters": "The 29-second timeout limit is a critical API Gateway constraint that affects architectural decisions. Long-running operations cannot be implemented synchronously through API Gateway. Understanding this limit prevents building systems that will fail in production and guides developers toward appropriate asynchronous patterns for time-consuming operations like file processing, report generation, or batch jobs.",
                  "key_takeaway": "API Gateway REST APIs have a maximum 29-second integration timeout—use asynchronous patterns for operations requiring longer processing times.",
                  "option_explanations": {
                    "A": "Lambda timeout would cause the Lambda to error, but API Gateway's 29-second limit is reached first.",
                    "B": "API Gateway REST APIs have a hard 29-second integration timeout limit that cannot be increased.",
                    "C": "Lambda throttling produces 429 errors, not 504 Gateway Timeout errors.",
                    "D": "Body size limit violations produce 413 Payload Too Large errors, not 504 timeout errors."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "domain:4", "service:api-gateway", "timeout", "limits"]
                },
                {
                  "id": "apigw-int-006",
                  "concept_id": "content-type-mapping",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "An API Gateway REST API needs to apply different mapping templates based on the Content-Type header of incoming requests. How can this be implemented?",
                  "options": [
                    {"label": "A", "text": "Create multiple methods for each Content-Type"},
                    {"label": "B", "text": "Configure content-type specific mapping templates in the integration request"},
                    {"label": "C", "text": "Use a Lambda function to detect Content-Type and route accordingly"},
                    {"label": "D", "text": "Content-Type cannot be used to select different mapping templates"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "API Gateway allows configuring multiple mapping templates in the integration request, each associated with a specific content-type (like application/json, application/xml, etc.). API Gateway automatically selects the appropriate template based on the request's Content-Type header. This enables handling different request formats with a single API method. Creating multiple methods is unnecessary overhead. Lambda-based routing adds latency and complexity. Content-type-based template selection is a native API Gateway feature.",
                  "why_this_matters": "Supporting multiple content types is common in APIs that serve diverse clients or integrate with legacy systems. API Gateway's content-type-based template selection enables elegant handling of JSON, XML, and other formats without code duplication or complex routing logic. This feature is essential for building flexible APIs that accommodate different client capabilities and protocols.",
                  "key_takeaway": "Configure content-type-specific mapping templates in API Gateway integration requests to automatically handle different request formats based on the Content-Type header.",
                  "option_explanations": {
                    "A": "Multiple methods are unnecessary; content-type-specific mapping templates handle this within a single method.",
                    "B": "Integration request supports multiple mapping templates keyed by content-type, automatically selecting the right one.",
                    "C": "Lambda routing adds unnecessary complexity when API Gateway natively supports content-type-based template selection.",
                    "D": "Content-Type-based mapping template selection is a native API Gateway feature for handling multiple formats."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "service:api-gateway", "content-type", "mapping-templates"]
                },
                {
                  "id": "apigw-int-007",
                  "concept_id": "passthrough-behavior",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "An API Gateway REST API receives a request with Content-Type: text/plain but only has mapping templates configured for application/json. The passthrough behavior is set to WHEN_NO_MATCH. What will happen?",
                  "options": [
                    {"label": "A", "text": "API Gateway will reject the request with a 415 Unsupported Media Type error"},
                    {"label": "B", "text": "API Gateway will pass the request through to the backend without transformation"},
                    {"label": "C", "text": "API Gateway will apply the application/json template anyway"},
                    {"label": "D", "text": "API Gateway will return a 400 Bad Request error"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "When passthrough behavior is set to WHEN_NO_MATCH and no matching content-type template exists, API Gateway passes the request through to the backend without applying any mapping template transformation. WHEN_NO_TEMPLATES would reject with 415. NEVER would reject with 415 when no match is found. The request passes through as-is, allowing the backend to handle it. API Gateway doesn't apply mismatched templates or return 400 errors for content-type mismatches.",
                  "why_this_matters": "Understanding passthrough behavior is important for building flexible APIs that handle unexpected content types gracefully. WHEN_NO_MATCH allows backends to handle content types not explicitly configured in API Gateway, providing flexibility. WHEN_NO_TEMPLATES or NEVER enforce strict content-type checking. Choosing the right passthrough behavior affects API flexibility and error handling.",
                  "key_takeaway": "Passthrough behavior WHEN_NO_MATCH allows requests with unconfigured content types to pass through untransformed; use NEVER to strictly enforce configured content types.",
                  "option_explanations": {
                    "A": "WHEN_NO_MATCH doesn't reject requests; it passes them through without transformation when no matching template exists.",
                    "B": "WHEN_NO_MATCH passthrough behavior sends requests without matching templates directly to the backend untransformed.",
                    "C": "API Gateway doesn't apply mismatched templates; behavior depends on passthrough configuration.",
                    "D": "Content-type mismatches with WHEN_NO_MATCH result in passthrough, not 400 errors."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "service:api-gateway", "passthrough-behavior", "content-type"]
                },
                {
                  "id": "apigw-int-008",
                  "concept_id": "response-mapping",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "An API Gateway REST API uses AWS integration to call DynamoDB. The DynamoDB response needs to be transformed to match the API's response schema. Where should the developer configure this transformation?",
                  "options": [
                    {"label": "A", "text": "Integration request mapping template"},
                    {"label": "B", "text": "Integration response mapping template"},
                    {"label": "C", "text": "Method request"},
                    {"label": "D", "text": "Method response"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Integration response mapping templates transform the backend's response before it's returned to the client. Integration request templates transform the incoming request before sending to the backend. Method request defines request parameters and validation. Method response defines the response structure that clients see. To transform DynamoDB's response format, use integration response mapping templates.",
                  "why_this_matters": "Understanding the distinction between integration and method request/response is fundamental to API Gateway configuration. Integration components handle backend communication and transformation, while method components define the API contract with clients. Response transformation in integration response enables clean API contracts that abstract backend implementation details from clients.",
                  "key_takeaway": "Use integration response mapping templates to transform backend responses before returning to clients; integration request templates transform client requests before backend invocation.",
                  "option_explanations": {
                    "A": "Integration request templates transform client requests before backend calls, not backend responses.",
                    "B": "Integration response templates transform backend responses before returning to clients, the correct location for this transformation.",
                    "C": "Method request defines request parameters and validation, not response transformation.",
                    "D": "Method response defines the client-facing response schema but doesn't perform transformation from backend format."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "service:api-gateway", "service:dynamodb", "response-mapping", "transformation"]
                },
                {
                  "id": "apigw-int-009",
                  "concept_id": "vpc-link-integration",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "An API Gateway REST API needs to integrate with a private Application Load Balancer in a VPC. What must be configured to enable this integration?",
                  "options": [
                    {"label": "A", "text": "VPC endpoint for API Gateway"},
                    {"label": "B", "text": "VPC Link to connect API Gateway to the VPC resource"},
                    {"label": "C", "text": "Direct VPC integration in API Gateway settings"},
                    {"label": "D", "text": "Make the ALB publicly accessible"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "VPC Link enables API Gateway REST APIs to access private HTTP resources like ALBs, NLBs, or EC2 instances in VPCs. The VPC Link uses a Network Load Balancer to bridge API Gateway (which runs outside your VPC) to private VPC resources. VPC endpoints are for services in your VPC to reach API Gateway, not the reverse. API Gateway doesn't have direct VPC integration. Making the ALB public defeats the purpose of VPC privacy.",
                  "why_this_matters": "VPC Link is essential for building APIs that front private backend services without exposing them to the internet. This pattern is common for microservices architectures where API Gateway provides a public interface while backend services remain private and secure. Understanding VPC Link enables proper architectural separation between public APIs and private implementation layers.",
                  "key_takeaway": "Use VPC Link to connect API Gateway REST APIs to private HTTP resources in VPCs (ALB, NLB, EC2) without exposing backend services to the internet.",
                  "option_explanations": {
                    "A": "VPC endpoints allow VPC resources to reach API Gateway, not the reverse direction needed here.",
                    "B": "VPC Link enables API Gateway to securely access private HTTP resources in VPCs.",
                    "C": "API Gateway doesn't have direct VPC integration; VPC Link is required for private resource access.",
                    "D": "Making the ALB public exposes backend services unnecessarily and defeats VPC security."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "service:api-gateway", "service:vpc", "vpc-link", "private-integration"]
                },
                {
                  "id": "apigw-int-010",
                  "concept_id": "parameter-mapping",
                  "variant_index": 0,
                  "topic": "api-gateway",
                  "subtopic": "api-gateway-integration",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "multi",
                  "stem": "An API Gateway method receives a request with query parameter 'userId'. The backend Lambda function expects this value in a JSON body field called 'user_id'. Which TWO configurations are needed? (Select TWO)",
                  "options": [
                    {"label": "A", "text": "Define userId as a query string parameter in method request"},
                    {"label": "B", "text": "Create an integration request mapping template that maps query parameter to JSON body"},
                    {"label": "C", "text": "Configure a request validator"},
                    {"label": "D", "text": "Use Lambda proxy integration"}
                  ],
                  "correct_options": ["A", "B"],
                  "answer_explanation": "First, define the query parameter in method request to make it available to API Gateway. Then, create an integration request mapping template that extracts the query parameter value and places it in the JSON body with the desired field name. Request validators check parameter presence/format but don't transform data. Lambda proxy integration would receive the query parameter as-is in the event object without transformation, requiring Lambda code to handle the mapping.",
                  "why_this_matters": "Parameter mapping is fundamental to bridging differences between API contracts and backend implementations. Integration mapping templates enable transforming parameter locations (query to body, header to body, etc.) and names without changing backend code. This decoupling allows evolving APIs independently of backend implementations and integrating with services that have different parameter conventions.",
                  "key_takeaway": "Use method request parameter definitions combined with integration request mapping templates to transform parameter locations and names between client requests and backend expectations.",
                  "option_explanations": {
                    "A": "Method request parameter definition makes the query parameter available for mapping.",
                    "B": "Integration request mapping template transforms the query parameter into the JSON body field.",
                    "C": "Request validators check data but don't transform parameter locations or names.",
                    "D": "Lambda proxy sends query parameters as-is in the event; transformation would be needed in Lambda code, not API Gateway."
                  },
                  "tags": ["topic:api-gateway", "subtopic:api-gateway-integration", "domain:1", "service:api-gateway", "parameter-mapping", "transformation"]
                }
              ]
            }
          ]
        },
        {
          "topic_id": "s3",
          "name": "Amazon S3",
          "subtopics": [
            {
              "subtopic_id": "s3-security",
              "name": "S3 security, encryption, and access control",
              "num_questions_generated": 10,
              "questions": [
                {
                  "id": "s3-sec-001",
                  "concept_id": "s3-bucket-policies",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A developer needs to grant a Lambda function access to read objects from an S3 bucket. What is the MOST secure approach?",
                  "options": [
                    {"label": "A", "text": "Create an IAM user with S3 read permissions and store the access keys in the Lambda environment variables"},
                    {"label": "B", "text": "Make the S3 bucket publicly readable"},
                    {"label": "C", "text": "Attach an IAM role to the Lambda function with S3 read permissions"},
                    {"label": "D", "text": "Use a bucket policy to allow public read access"}
                  ],
                  "correct_options": ["C"],
                  "answer_explanation": "IAM roles attached to Lambda functions provide temporary credentials automatically managed by AWS, following security best practices. The role grants only the necessary S3 permissions without exposing long-term credentials. Storing IAM user access keys in environment variables is a security anti-pattern that risks credential exposure. Making buckets publicly readable exposes data unnecessarily and violates least privilege. Bucket policies allowing public access create security vulnerabilities.",
                  "why_this_matters": "Proper IAM role usage is fundamental to AWS security. Roles provide temporary credentials that automatically rotate, eliminating the risks of long-term credential management. This pattern prevents credential leakage, supports audit trails through CloudTrail, and enforces least privilege access. Understanding role-based access versus credential-based access is essential for building secure AWS applications.",
                  "key_takeaway": "Always use IAM roles for AWS service-to-service access like Lambda to S3—never embed long-term IAM user credentials in code or environment variables.",
                  "option_explanations": {
                    "A": "Storing IAM user credentials in environment variables is a critical security vulnerability.",
                    "B": "Public bucket access exposes data unnecessarily and violates security best practices.",
                    "C": "IAM roles provide temporary, automatically-managed credentials following security best practices.",
                    "D": "Public bucket policies create security risks by exposing data to the internet."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "service:iam", "security", "roles"]
                },
                {
                  "id": "s3-sec-002",
                  "concept_id": "s3-encryption-at-rest",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A company requires that all S3 objects be encrypted at rest using AWS KMS keys that they manage. Which S3 encryption option meets this requirement?",
                  "options": [
                    {"label": "A", "text": "SSE-S3 (Server-Side Encryption with S3-Managed Keys)"},
                    {"label": "B", "text": "SSE-KMS (Server-Side Encryption with AWS KMS)"},
                    {"label": "C", "text": "SSE-C (Server-Side Encryption with Customer-Provided Keys)"},
                    {"label": "D", "text": "Client-side encryption"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "SSE-KMS uses AWS KMS customer managed keys (CMKs) that you control, providing key management capabilities, rotation, and CloudTrail audit logging of key usage. SSE-S3 uses AWS-managed keys you don't control. SSE-C requires providing keys with each request, not managing them in AWS. Client-side encryption requires managing encryption in application code. SSE-KMS is the correct choice for company-managed keys within AWS.",
                  "why_this_matters": "Different S3 encryption options provide different levels of control and audit capabilities. SSE-KMS is essential for compliance requirements demanding customer-controlled keys, key usage auditing, and key rotation policies. Understanding encryption options helps meet regulatory requirements while balancing operational complexity and security controls.",
                  "key_takeaway": "Use SSE-KMS when you need to manage encryption keys, audit key usage, or meet compliance requirements for customer-controlled encryption keys in AWS.",
                  "option_explanations": {
                    "A": "SSE-S3 uses AWS-managed keys that customers don't control or manage.",
                    "B": "SSE-KMS uses customer-managed KMS keys providing control, rotation, and audit logging.",
                    "C": "SSE-C requires providing keys with each request, not storing them in AWS for management.",
                    "D": "Client-side encryption requires application-level key management, not AWS-managed keys."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "service:kms", "encryption", "sse-kms"]
                },
                {
                  "id": "s3-sec-003",
                  "concept_id": "s3-presigned-urls",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A web application needs to allow users to upload files directly to S3 without exposing AWS credentials to the browser. What is the BEST approach?",
                  "options": [
                    {"label": "A", "text": "Make the S3 bucket publicly writable"},
                    {"label": "B", "text": "Generate presigned URLs from the backend that grant temporary upload permission"},
                    {"label": "C", "text": "Embed IAM access keys in the JavaScript code"},
                    {"label": "D", "text": "Use S3 bucket policies to allow uploads from the website domain"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Presigned URLs are generated by the backend using IAM credentials and provide temporary, limited-scope access to upload specific objects to S3. The URL contains embedded authentication and expires after a set time, eliminating the need to expose credentials to browsers. Public write buckets create severe security risks. Embedding credentials in JavaScript exposes them to anyone viewing the page source. Bucket policies can't securely authenticate individual users for direct browser uploads.",
                  "why_this_matters": "Presigned URLs are the standard pattern for secure direct browser-to-S3 uploads. They enable client-side uploads without backend proxying (reducing bandwidth costs and latency) while maintaining security through time-limited, scoped permissions. This pattern is essential for file upload features in modern web applications.",
                  "key_takeaway": "Use presigned URLs to enable secure, temporary, direct client uploads to S3 without exposing AWS credentials or making buckets publicly writable.",
                  "option_explanations": {
                    "A": "Publicly writable buckets create severe security vulnerabilities allowing anyone to upload anything.",
                    "B": "Presigned URLs provide temporary, scoped upload permissions without exposing credentials.",
                    "C": "Embedding credentials in client-side code exposes them to anyone, a critical security flaw.",
                    "D": "Bucket policies alone can't securely authenticate individual users for browser uploads."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "presigned-urls", "security"]
                },
                {
                  "id": "s3-sec-004",
                  "concept_id": "s3-block-public-access",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A security audit reveals that several S3 buckets have public access enabled. What S3 feature can prevent future accidental public bucket exposure at the account level?",
                  "options": [
                    {"label": "A", "text": "S3 versioning"},
                    {"label": "B", "text": "S3 Block Public Access settings"},
                    {"label": "C", "text": "S3 Object Lock"},
                    {"label": "D", "text": "S3 bucket policies"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "S3 Block Public Access provides centralized controls to prevent public access at the account or bucket level, overriding bucket policies and ACLs that would otherwise grant public access. This feature acts as a safeguard against accidental public exposure. Versioning protects against deletion, not public access. Object Lock prevents deletion/modification, not public access. Bucket policies can grant access but don't prevent future public configurations.",
                  "why_this_matters": "Accidental public S3 bucket exposure is a common cause of data breaches. Block Public Access provides a safety net that prevents public access even if bucket policies or ACLs are misconfigured. Enabling this at the account level is a security best practice that reduces the risk of data exposure from configuration mistakes.",
                  "key_takeaway": "Enable S3 Block Public Access at the account level to prevent accidental public bucket exposure regardless of bucket policies or ACLs.",
                  "option_explanations": {
                    "A": "Versioning protects against data deletion, not public access configuration.",
                    "B": "Block Public Access prevents public bucket access at account or bucket level, overriding other settings.",
                    "C": "Object Lock prevents object deletion/modification, not public access configuration.",
                    "D": "Bucket policies can grant access but don't prevent future public access configurations."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "block-public-access", "security"]
                },
                {
                  "id": "s3-sec-005",
                  "concept_id": "s3-encryption-in-transit",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A company's compliance policy requires that all data transfers to S3 must be encrypted in transit. How can this be enforced?",
                  "options": [
                    {"label": "A", "text": "Enable S3 default encryption"},
                    {"label": "B", "text": "Create a bucket policy that denies requests where aws:SecureTransport is false"},
                    {"label": "C", "text": "Use SSE-KMS encryption"},
                    {"label": "D", "text": "Enable S3 versioning"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "A bucket policy with a Deny statement for aws:SecureTransport = false rejects any requests not using HTTPS/TLS, enforcing encryption in transit. Default encryption and SSE-KMS encrypt data at rest, not in transit. Versioning protects against deletion but doesn't enforce transit encryption. The SecureTransport condition key is specifically designed to enforce HTTPS usage.",
                  "why_this_matters": "Encryption in transit protects data from interception during transmission. While AWS SDK and console use HTTPS by default, bucket policies can enforce this requirement preventing accidental or intentional HTTP usage. This is critical for compliance requirements mandating end-to-end encryption and protecting sensitive data from network-level attacks.",
                  "key_takeaway": "Use bucket policies with aws:SecureTransport condition to enforce HTTPS and prevent unencrypted data transfers to S3.",
                  "option_explanations": {
                    "A": "Default encryption protects data at rest, not in transit during upload/download.",
                    "B": "Bucket policy with SecureTransport condition enforces HTTPS usage, encrypting data in transit.",
                    "C": "SSE-KMS encrypts data at rest on S3, not during transmission.",
                    "D": "Versioning protects against deletion/overwriting, not transit encryption."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "encryption-in-transit", "bucket-policy"]
                },
                {
                  "id": "s3-sec-006",
                  "concept_id": "s3-access-logs",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A security team needs to audit all access to objects in an S3 bucket. Which feature should they enable?",
                  "options": [
                    {"label": "A", "text": "S3 Versioning"},
                    {"label": "B", "text": "S3 Server Access Logging"},
                    {"label": "C", "text": "S3 Lifecycle policies"},
                    {"label": "D", "text": "S3 Object Lock"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "S3 Server Access Logging records detailed information about requests made to a bucket, including requester, bucket name, request time, action, response status, and error codes. These logs are delivered to a target bucket for analysis. Versioning tracks object versions, not access. Lifecycle policies manage object retention. Object Lock prevents deletion. For comprehensive API-level auditing including who accessed via IAM, CloudTrail is also used, but for bucket-level access patterns, Server Access Logging is the S3-native solution.",
                  "why_this_matters": "Access logging is essential for security auditing, compliance, and troubleshooting access issues. It provides visibility into who accesses what data and when, enabling detection of unauthorized access, data exfiltration attempts, and usage pattern analysis. This is a fundamental security control for sensitive data in S3.",
                  "key_takeaway": "Enable S3 Server Access Logging to audit and track all access requests to S3 buckets for security monitoring and compliance.",
                  "option_explanations": {
                    "A": "Versioning tracks object versions over time, not who accesses objects.",
                    "B": "Server Access Logging records detailed access request information for auditing and security analysis.",
                    "C": "Lifecycle policies manage object storage class transitions and deletion, not access auditing.",
                    "D": "Object Lock prevents object deletion/modification, not access tracking."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "access-logging", "auditing"]
                },
                {
                  "id": "s3-sec-007",
                  "concept_id": "s3-cors",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-1-development",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A static website hosted on S3 at website.com needs to make API calls to an API Gateway endpoint at api.example.com from browser JavaScript. The browser is blocking the requests. What must be configured?",
                  "options": [
                    {"label": "A", "text": "Enable S3 versioning"},
                    {"label": "B", "text": "Configure CORS on the API Gateway to allow requests from website.com"},
                    {"label": "C", "text": "Configure CORS on the S3 bucket"},
                    {"label": "D", "text": "Make the S3 bucket publicly readable"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Cross-Origin Resource Sharing (CORS) must be configured on the resource being accessed (API Gateway), not the origin (S3 website). The API Gateway needs CORS headers allowing requests from website.com. CORS on S3 would be needed if external websites were accessing S3 objects directly. Versioning and public read access don't affect cross-origin browser restrictions.",
                  "why_this_matters": "Understanding CORS is essential for building web applications that make cross-origin requests. CORS is configured on the destination resource, not the source. Misconfiguring CORS is a common issue preventing frontend-backend communication in distributed applications. Knowing where to configure CORS prevents hours of troubleshooting browser security errors.",
                  "key_takeaway": "Configure CORS on the destination resource (API Gateway, S3) to allow cross-origin requests from browsers; CORS headers must be sent by the resource being accessed.",
                  "option_explanations": {
                    "A": "Versioning is unrelated to cross-origin request permissions.",
                    "B": "API Gateway needs CORS configuration to allow cross-origin requests from the website's domain.",
                    "C": "CORS on S3 would help if accessing S3 objects from another domain, not for S3 calling API Gateway.",
                    "D": "Public read access affects authorization, not cross-origin request permissions."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:1", "service:s3", "service:api-gateway", "cors", "web-development"]
                },
                {
                  "id": "s3-sec-008",
                  "concept_id": "s3-object-ownership",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "An S3 bucket receives objects uploaded by multiple AWS accounts. The bucket owner needs full control over all objects regardless of who uploaded them. What S3 feature ensures this?",
                  "options": [
                    {"label": "A", "text": "S3 Versioning"},
                    {"label": "B", "text": "S3 Object Ownership set to 'Bucket owner enforced'"},
                    {"label": "C", "text": "S3 Bucket Policies"},
                    {"label": "D", "text": "S3 Access Control Lists (ACLs)"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "S3 Object Ownership with 'Bucket owner enforced' setting disables ACLs and ensures the bucket owner automatically owns and has full control over all objects in the bucket, regardless of which account uploaded them. This simplifies permission management for multi-account scenarios. Versioning protects against deletion. Bucket policies grant access but don't automatically transfer object ownership. ACLs can complicate ownership; Object Ownership setting disables them.",
                  "why_this_matters": "Object ownership is critical for centralized data management in multi-account architectures. Without proper ownership settings, uploaded objects might be controlled by uploading accounts, preventing bucket owners from managing or deleting them. Object Ownership settings simplify permission management and prevent orphaned objects that bucket owners cannot control.",
                  "key_takeaway": "Use S3 Object Ownership 'Bucket owner enforced' to ensure the bucket owner controls all objects regardless of uploader, simplifying multi-account permission management.",
                  "option_explanations": {
                    "A": "Versioning protects against deletion but doesn't affect object ownership.",
                    "B": "Object Ownership 'Bucket owner enforced' ensures bucket owner controls all objects automatically.",
                    "C": "Bucket policies control access but don't automatically change object ownership.",
                    "D": "ACLs can complicate ownership; Object Ownership setting disables them for simplified management."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "object-ownership", "multi-account"]
                },
                {
                  "id": "s3-sec-009",
                  "concept_id": "s3-mfa-delete",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A company stores critical financial records in S3 and wants to prevent accidental deletion even by users with delete permissions. What S3 feature provides an additional layer of protection?",
                  "options": [
                    {"label": "A", "text": "S3 Versioning with MFA Delete"},
                    {"label": "B", "text": "S3 Lifecycle policies"},
                    {"label": "C", "text": "S3 Server Access Logging"},
                    {"label": "D", "text": "S3 Bucket Policies"}
                  ],
                  "correct_options": ["A"],
                  "answer_explanation": "MFA Delete requires multi-factor authentication for permanently deleting object versions or changing the versioning state of the bucket. This adds a strong protection layer against accidental or malicious deletion. Versioning alone allows deletion of versions. Lifecycle policies automate transitions/deletions but don't prevent them. Logging tracks deletions but doesn't prevent them. Bucket policies can restrict deletion but MFA Delete adds authentication-based protection.",
                  "why_this_matters": "MFA Delete provides strong protection for critical data by requiring physical device authentication for destructive operations. This prevents both accidental deletion by authorized users and malicious deletion if credentials are compromised. For compliance and data retention requirements, MFA Delete is an essential control for irreplaceable data.",
                  "key_takeaway": "Enable S3 Versioning with MFA Delete to require multi-factor authentication for permanent deletion, adding strong protection against accidental or malicious data loss.",
                  "option_explanations": {
                    "A": "MFA Delete requires multi-factor authentication for permanent deletion, providing strong protection.",
                    "B": "Lifecycle policies automate deletions but don't prevent or require approval for manual deletions.",
                    "C": "Access logging records deletions but doesn't prevent them.",
                    "D": "Bucket policies can restrict delete permissions but MFA Delete adds authentication-based protection."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "mfa-delete", "versioning", "data-protection"]
                },
                {
                  "id": "s3-sec-010",
                  "concept_id": "s3-kms-permissions",
                  "variant_index": 0,
                  "topic": "s3",
                  "subtopic": "s3-security",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "hard",
                  "question_type": "multi",
                  "stem": "A Lambda function needs to read S3 objects encrypted with SSE-KMS. Which TWO permissions are required in the Lambda execution role? (Select TWO)",
                  "options": [
                    {"label": "A", "text": "s3:GetObject on the S3 bucket"},
                    {"label": "B", "text": "kms:Decrypt on the KMS key"},
                    {"label": "C", "text": "s3:PutObject on the S3 bucket"},
                    {"label": "D", "text": "kms:Encrypt on the KMS key"}
                  ],
                  "correct_options": ["A", "B"],
                  "answer_explanation": "Reading SSE-KMS encrypted objects requires both s3:GetObject permission to retrieve the object from S3 and kms:Decrypt permission to decrypt the object using the KMS key. S3 automatically decrypts objects when retrieving them if you have both permissions. PutObject is for writing objects, not reading. kms:Encrypt is for encrypting new data, not decrypting existing data.",
                  "why_this_matters": "SSE-KMS encryption adds an additional permission layer beyond S3 permissions. Applications must have both S3 read permissions and KMS decrypt permissions to access encrypted objects. Forgetting KMS permissions is a common mistake causing access denied errors even when S3 permissions are correct. Understanding this dual-permission requirement is essential for KMS-encrypted data access.",
                  "key_takeaway": "Accessing SSE-KMS encrypted S3 objects requires both S3 read permissions (s3:GetObject) and KMS decrypt permissions (kms:Decrypt) on the encryption key.",
                  "option_explanations": {
                    "A": "s3:GetObject permission is required to retrieve objects from S3.",
                    "B": "kms:Decrypt permission is required to decrypt SSE-KMS encrypted objects.",
                    "C": "s3:PutObject is for writing objects, not reading them.",
                    "D": "kms:Encrypt is for encrypting new data, not decrypting existing encrypted objects."
                  },
                  "tags": ["topic:s3", "subtopic:s3-security", "domain:2", "service:s3", "service:kms", "sse-kms", "permissions"]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "domain_id": "domain-2-security",
      "name": "Security",
      "topics": [
        {
          "topic_id": "iam",
          "name": "AWS Identity and Access Management",
          "subtopics": [
            {
              "subtopic_id": "iam-roles-policies",
              "name": "IAM roles, policies, and least privilege",
              "num_questions_generated": 10,
              "questions": [
                {
                  "id": "iam-rp-001",
                  "concept_id": "iam-role-vs-user",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A developer needs to grant an EC2 instance permission to access DynamoDB. What is the MOST secure approach?",
                  "options": [
                    {"label": "A", "text": "Create an IAM user, generate access keys, and store them in a file on the EC2 instance"},
                    {"label": "B", "text": "Attach an IAM role to the EC2 instance with DynamoDB permissions"},
                    {"label": "C", "text": "Hard-code access keys in the application code"},
                    {"label": "D", "text": "Use the root account credentials"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "IAM roles provide temporary credentials that are automatically rotated and managed by AWS. When attached to EC2 instances, applications can access AWS services without storing long-term credentials. Storing access keys on instances or in code risks credential exposure and requires manual rotation. Root account credentials should never be used for applications. Roles eliminate credential management burden while providing better security.",
                  "why_this_matters": "IAM roles are fundamental to AWS security best practices. They eliminate the need to manage, rotate, and secure long-term credentials on compute resources. Credentials stored on instances or in code can be exposed through various attack vectors including instance compromise, code repository leaks, or log files. Roles provide automatic credential rotation and integration with AWS audit tools.",
                  "key_takeaway": "Always use IAM roles for AWS compute services (EC2, Lambda, ECS) to access other AWS services—never store long-term credentials on instances or in code.",
                  "option_explanations": {
                    "A": "Storing access keys on instances creates security risks from credential exposure and requires manual rotation.",
                    "B": "IAM roles provide automatically-rotated temporary credentials, the secure solution for service-to-service access.",
                    "C": "Hard-coding credentials is a critical security vulnerability that exposes credentials in code repositories.",
                    "D": "Root account credentials should never be used for applications and lack fine-grained permissions."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "service:ec2", "roles", "security"]
                },
                {
                  "id": "iam-rp-002",
                  "concept_id": "least-privilege-principle",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A Lambda function only needs to read items from a specific DynamoDB table named 'Users'. Which IAM policy follows the principle of least privilege?",
                  "options": [
                    {"label": "A", "text": "A policy granting dynamodb:* on all resources"},
                    {"label": "B", "text": "A policy granting dynamodb:GetItem and dynamodb:Query on the 'Users' table ARN"},
                    {"label": "C", "text": "A policy granting dynamodb:GetItem on all DynamoDB tables"},
                    {"label": "D", "text": "The AdministratorAccess managed policy"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Least privilege means granting only the minimum permissions necessary. The function needs only read operations (GetItem, Query) on the specific 'Users' table. Granting all DynamoDB actions or access to all tables violates least privilege. Administrator access grants far more permissions than needed. Scoping permissions to specific actions and resources minimizes potential damage from security breaches or bugs.",
                  "why_this_matters": "Least privilege is a fundamental security principle that limits blast radius from security incidents. If a function with broad permissions is compromised, attackers gain extensive access. Narrowly scoped permissions contain potential damage. This principle is essential for compliance, security audits, and defense-in-depth strategies. Over-permissioned roles are a common security vulnerability.",
                  "key_takeaway": "Follow least privilege by granting only specific actions (GetItem, Query) on specific resources (table ARNs) rather than broad permissions like wildcards or AdministratorAccess.",
                  "option_explanations": {
                    "A": "Wildcard permissions on all actions and resources violate least privilege, granting unnecessary permissions.",
                    "B": "Grants only required read actions on the specific table, following least privilege principle.",
                    "C": "Access to all tables grants more permission than needed for accessing one table.",
                    "D": "AdministratorAccess grants permissions far beyond DynamoDB read access, severely violating least privilege."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "least-privilege", "policies"]
                },
                {
                  "id": "iam-rp-003",
                  "concept_id": "policy-evaluation-logic",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "An IAM role has an identity-based policy allowing s3:GetObject on all S3 buckets. The S3 bucket has a resource-based policy with an explicit Deny for the role's principal. What is the effective permission?",
                  "options": [
                    {"label": "A", "text": "Allow - the identity policy takes precedence"},
                    {"label": "B", "text": "Deny - explicit denies always override allows"},
                    {"label": "C", "text": "Allow - resource policies don't affect IAM roles"},
                    {"label": "D", "text": "Neither - the policies conflict and cancel out"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "In AWS IAM policy evaluation, explicit Deny statements always take precedence over Allow statements, regardless of where they appear (identity policies, resource policies, SCPs, or permission boundaries). Even if an identity policy allows an action, an explicit Deny in any applicable policy will prevent the action. This evaluation logic ensures that deny statements cannot be overridden, providing a strong security control.",
                  "why_this_matters": "Understanding IAM policy evaluation logic is critical for troubleshooting permissions and implementing secure architectures. The deny-override rule provides a mechanism to block access regardless of other allows, useful for compliance and security controls. Misunderstanding evaluation logic leads to permission issues and potential security gaps in access control implementations.",
                  "key_takeaway": "Explicit Deny statements in IAM policies always override Allow statements regardless of policy type or location—use Deny for security controls that must not be overridden.",
                  "option_explanations": {
                    "A": "Identity policies don't take precedence over explicit Deny statements in resource policies.",
                    "B": "Explicit Deny always overrides any Allow, regardless of policy type or source.",
                    "C": "Resource policies absolutely affect IAM roles; explicit Denies override identity policy Allows.",
                    "D": "Policies don't cancel out; explicit Deny takes precedence in IAM evaluation logic."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "policy-evaluation", "deny"]
                },
                {
                  "id": "iam-rp-004",
                  "concept_id": "iam-policy-variables",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "A multi-tenant application needs to ensure users can only access S3 objects in folders matching their username. What IAM policy technique accomplishes this with a single policy?",
                  "options": [
                    {"label": "A", "text": "Create separate policies for each user"},
                    {"label": "B", "text": "Use IAM policy variables like ${aws:username} in the resource ARN"},
                    {"label": "C", "text": "Use bucket policies to grant access per user"},
                    {"label": "D", "text": "Grant access to the entire bucket and filter in application code"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "IAM policy variables allow creating dynamic policies where resource ARNs incorporate runtime values like ${aws:username}. A single policy can grant each user access to s3:*/home/${aws:username}/* without creating per-user policies. This scales efficiently for thousands of users. Creating separate policies doesn't scale. Bucket policies have size limits and don't scale well per-user. Application filtering requires overly broad IAM permissions.",
                  "why_this_matters": "Policy variables enable scalable, maintainable multi-tenant access control patterns. Without variables, managing individual policies for thousands of users becomes operationally infeasible. Variables allow single policies to dynamically adapt to individual principals, essential for SaaS applications, user home directories, and multi-tenant architectures requiring user isolation.",
                  "key_takeaway": "Use IAM policy variables like ${aws:username} to create dynamic, scalable policies that adapt to individual principals without requiring separate policies for each user.",
                  "option_explanations": {
                    "A": "Creating per-user policies doesn't scale and creates management overhead for large user bases.",
                    "B": "Policy variables enable one policy to dynamically scope access per user using runtime values.",
                    "C": "Bucket policies have size limits and per-user management doesn't scale effectively.",
                    "D": "Application-level filtering requires overly broad IAM permissions, violating least privilege."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "policy-variables", "multi-tenant"]
                },
                {
                  "id": "iam-rp-005",
                  "concept_id": "cross-account-access",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A Lambda function in Account A needs to access an S3 bucket in Account B. What is the MOST secure approach?",
                  "options": [
                    {"label": "A", "text": "Make the S3 bucket in Account B publicly accessible"},
                    {"label": "B", "text": "Create an IAM role in Account B that Account A can assume, then configure Lambda to assume that role"},
                    {"label": "C", "text": "Share IAM user credentials from Account B with Account A"},
                    {"label": "D", "text": "Copy the data to a bucket in Account A"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Cross-account access uses IAM role assumption where Account B creates a role with a trust policy allowing Account A to assume it. Account A's Lambda assumes the role to get temporary credentials for accessing Account B's resources. This maintains security boundaries and audit trails. Public buckets expose data unnecessarily. Sharing credentials violates security best practices. Copying data creates data synchronization and consistency issues.",
                  "why_this_matters": "Cross-account access is fundamental in multi-account AWS architectures used for organizational separation, security isolation, and compliance. Role assumption provides secure, auditable cross-account access without sharing long-term credentials. This pattern enables centralized services accessing resources across accounts while maintaining security boundaries and proper access controls.",
                  "key_takeaway": "Use IAM role assumption for secure cross-account access—create a role in the target account with a trust policy allowing the source account to assume it.",
                  "option_explanations": {
                    "A": "Public bucket access exposes data to the internet, not a secure cross-account solution.",
                    "B": "Role assumption provides secure, auditable cross-account access using temporary credentials.",
                    "C": "Sharing credentials violates security best practices and eliminates audit trails.",
                    "D": "Data copying creates synchronization issues and doesn't provide ongoing access to source data."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "cross-account", "role-assumption"]
                },
                {
                  "id": "iam-rp-006",
                  "concept_id": "inline-vs-managed-policies",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A development team needs to grant the same set of DynamoDB permissions to multiple Lambda functions. What is the BEST approach for managing these permissions?",
                  "options": [
                    {"label": "A", "text": "Create inline policies for each Lambda execution role"},
                    {"label": "B", "text": "Create a customer managed policy and attach it to all Lambda execution roles"},
                    {"label": "C", "text": "Hard-code the permissions in Lambda code"},
                    {"label": "D", "text": "Use the AdministratorAccess managed policy"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Customer managed policies are reusable and can be attached to multiple roles, making permission management centralized and consistent. When permissions need updating, you modify one policy affecting all roles. Inline policies are embedded in individual roles, requiring updates to each role separately. Hard-coding permissions in code isn't possible. AdministratorAccess violates least privilege. Managed policies are the best practice for shared permissions.",
                  "why_this_matters": "Choosing between inline and managed policies affects maintainability and operational efficiency. Managed policies enable centralized permission management where one update applies to all attached principals. This reduces errors, ensures consistency, and simplifies compliance auditing. Inline policies are appropriate only for single-use, role-specific permissions that shouldn't be shared.",
                  "key_takeaway": "Use customer managed policies for permissions shared across multiple principals; reserve inline policies for role-specific permissions that shouldn't be reused.",
                  "option_explanations": {
                    "A": "Inline policies require updating each role individually, creating management overhead.",
                    "B": "Customer managed policies can be attached to multiple roles, centralizing permission management.",
                    "C": "Permissions cannot be hard-coded in code; IAM controls AWS service access.",
                    "D": "AdministratorAccess grants excessive permissions, violating least privilege."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "managed-policies", "policy-management"]
                },
                {
                  "id": "iam-rp-007",
                  "concept_id": "permission-boundaries",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "hard",
                  "question_type": "single",
                  "stem": "A company wants to allow developers to create IAM roles for their Lambda functions but prevent them from granting permissions beyond what their team should have. What IAM feature accomplishes this?",
                  "options": [
                    {"label": "A", "text": "Service Control Policies (SCPs)"},
                    {"label": "B", "text": "IAM Permission Boundaries"},
                    {"label": "C", "text": "IAM Policy Conditions"},
                    {"label": "D", "text": "Resource-based policies"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Permission boundaries set the maximum permissions an IAM entity can have, regardless of identity policies attached to it. Developers can create roles and attach policies, but the permission boundary limits the effective permissions to allowed actions. SCPs are for AWS Organizations and affect entire accounts. Policy conditions add constraints to permissions. Resource policies control access to resources. Permission boundaries are specifically designed for delegated administration scenarios.",
                  "why_this_matters": "Permission boundaries enable safe delegation of IAM administration. Without boundaries, users with IAM creation permissions could escalate their own privileges. Boundaries ensure created roles cannot exceed defined limits, enabling development teams to self-service IAM while maintaining security guardrails. This pattern is essential for organizations balancing agility with security governance.",
                  "key_takeaway": "Use IAM permission boundaries to set maximum permissions for roles, enabling safe delegation of IAM administration while preventing privilege escalation.",
                  "option_explanations": {
                    "A": "SCPs are organization-level controls affecting accounts, not individual role permission limits.",
                    "B": "Permission boundaries define maximum permissions for IAM entities, perfect for delegated administration.",
                    "C": "Policy conditions add constraints but don't set maximum permission limits across all policies.",
                    "D": "Resource policies control resource access, not maximum permissions for IAM entities."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "permission-boundaries", "delegated-administration"]
                },
                {
                  "id": "iam-rp-008",
                  "concept_id": "service-linked-roles",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "easy",
                  "question_type": "single",
                  "stem": "A developer enables AWS Auto Scaling for an application. AWS automatically creates an IAM role for Auto Scaling to manage EC2 instances. What type of role is this?",
                  "options": [
                    {"label": "A", "text": "User-created role"},
                    {"label": "B", "text": "Service-linked role"},
                    {"label": "C", "text": "Cross-account role"},
                    {"label": "D", "text": "Instance profile role"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "Service-linked roles are predefined by AWS services and automatically created when you enable certain features. They include permissions the service needs to act on your behalf. You cannot modify their policies. Auto Scaling, Elastic Beanstalk, and many other services use service-linked roles. User-created roles are manually defined. Cross-account roles are for accessing resources in other accounts. Instance profiles contain roles but aren't role types themselves.",
                  "why_this_matters": "Understanding service-linked roles prevents confusion when AWS creates roles automatically. These roles are tightly coupled to services and have precisely the permissions needed. You cannot modify their policies because AWS manages them to ensure the service works correctly. Recognizing service-linked roles prevents unnecessary troubleshooting when services create roles you didn't explicitly define.",
                  "key_takeaway": "Service-linked roles are automatically created and managed by AWS services with predefined permissions that cannot be modified—they simplify service setup and ensure correct permissions.",
                  "option_explanations": {
                    "A": "Service-linked roles are created automatically by AWS services, not manually by users.",
                    "B": "Service-linked roles are AWS-managed roles automatically created when enabling certain service features.",
                    "C": "Cross-account roles are for accessing resources in different accounts, not AWS service operations.",
                    "D": "Instance profiles contain roles for EC2 but service-linked roles are a distinct concept for AWS service operations."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "service-linked-roles"]
                },
                {
                  "id": "iam-rp-009",
                  "concept_id": "sts-assumerole",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "medium",
                  "question_type": "single",
                  "stem": "A Lambda function needs to temporarily assume a role in another account to access resources. Which AWS service API should the Lambda function call?",
                  "options": [
                    {"label": "A", "text": "IAM GetRole"},
                    {"label": "B", "text": "STS AssumeRole"},
                    {"label": "C", "text": "IAM CreateRole"},
                    {"label": "D", "text": "STS GetSessionToken"}
                  ],
                  "correct_options": ["B"],
                  "answer_explanation": "STS (Security Token Service) AssumeRole returns temporary security credentials for assuming an IAM role. This is the standard API for role assumption, including cross-account access. GetRole retrieves role information but doesn't provide credentials. CreateRole creates new roles. GetSessionToken returns temporary credentials for the current IAM user, not for assuming a different role. AssumeRole is specifically designed for role assumption scenarios.",
                  "why_this_matters": "STS AssumeRole is fundamental to dynamic credential management and cross-account access patterns. Understanding when and how to use AssumeRole enables building secure, temporary-credential-based architectures. This API is central to IAM role usage, federated access, and cross-account resource access, making it essential knowledge for AWS developers.",
                  "key_takeaway": "Use STS AssumeRole to obtain temporary credentials when assuming IAM roles, including cross-account access scenarios.",
                  "option_explanations": {
                    "A": "IAM GetRole retrieves role metadata but doesn't provide credentials for assuming the role.",
                    "B": "STS AssumeRole returns temporary credentials for assuming a role, the correct API for this scenario.",
                    "C": "IAM CreateRole creates new roles but doesn't provide credentials for using them.",
                    "D": "STS GetSessionToken gets temporary credentials for the current user, not for assuming a different role."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "service:sts", "assume-role"]
                },
                {
                  "id": "iam-rp-010",
                  "concept_id": "resource-based-vs-identity-based",
                  "variant_index": 0,
                  "topic": "iam",
                  "subtopic": "iam-roles-policies",
                  "domain": "domain-2-security",
                  "difficulty_inferred": "hard",
                  "question_type": "multi",
                  "stem": "A developer needs to grant a Lambda function in Account A access to an S3 bucket in Account B. Which TWO approaches will work? (Select TWO)",
                  "options": [
                    {"label": "A", "text": "Add a resource-based policy to the S3 bucket allowing the Lambda execution role from Account A"},
                    {"label": "B", "text": "Have the Lambda function assume a role in Account B that has S3 access"},
                    {"label": "C", "text": "Add an identity-based policy to the Lambda execution role in Account A granting S3 access"},
                    {"label": "D", "text": "Create an IAM user in Account B and use those credentials in Lambda"}
                  ],
                  "correct_options": ["A", "B"],
                  "answer_explanation": "Cross-account access can be achieved two ways: (1) resource-based policy on the bucket allowing Account A's role principal, or (2) role assumption where Lambda assumes an Account B role with S3 access. Identity-based policies in Account A cannot directly grant access to Account B resources without corresponding resource policies or role assumption. Using IAM user credentials violates security best practices. Both resource-based policies and role assumption are valid cross-account patterns.",
                  "why_this_matters": "Understanding both cross-account access patterns (resource policies and role assumption) provides flexibility in architecture design. Resource policies are simpler for single-resource access. Role assumption is better for accessing multiple resources or when the resource doesn't support resource policies. Knowing both approaches enables choosing the right pattern for specific requirements.",
                  "key_takeaway": "Cross-account access can use either resource-based policies allowing cross-account principals or role assumption—both are valid, choose based on access pattern and supported resource types.",
                  "option_explanations": {
                    "A": "Resource-based bucket policy can directly allow cross-account Lambda role access.",
                    "B": "Role assumption provides cross-account access through temporary credentials.",
                    "C": "Identity-based policies alone cannot grant cross-account access without resource policies or role assumption.",
                    "D": "Using IAM user credentials violates security best practices; use roles for service access."
                  },
                  "tags": ["topic:iam", "subtopic:iam-roles-policies", "domain:2", "service:iam", "cross-account", "resource-policy", "identity-policy"]
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}
